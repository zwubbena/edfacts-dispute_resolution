{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cq4Onbet2pqr",
        "3q1d8Sky21lN",
        "pOmvRCYQ26e_",
        "pBIx8H802_fF"
      ],
      "authorship_tag": "ABX9TyNAY0EvxsTjYsURIAtTcmpl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zwubbena/edfacts-dispute_resolution/blob/main/edfacts_dispute_resolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EDFacts IDEA Part B Dispute Resolution**\n",
        "\n",
        "## **EDFacts Submission in EDPass:**\n",
        "\n",
        "EDFacts Dispute Resolution data submissions in EDPass for the 2024-25 school year include four file specification (FS) documents:\n",
        "\n",
        "- **FS227** – IDEA Part B Dispute Resolution Written, Signed Complaints\n",
        "- **FS228** – IDEA Part B Dispute Resolution Mediation Requests\n",
        "- **FS229** – IDEA Part B Dispute Resolution Due Process Complaints\n",
        "- **FS230** – IDEA Part B Dispute Resolution Expedited Due Process Complaints\n",
        "\n",
        "\n",
        "**EDPass as a Submission Platform:** EDPass is the primary system used by State Education Agencies (SEAs) to upload and submit EDFacts data, including dispute resolution data, to the U.S. Department of Education. The new file specification documents were published in May 2025.\n",
        "\n",
        "## **State Performance Plan Indicators (SPPI):**\n",
        "\n",
        "The EDFacts data map to the two indicators in the State Performance Plan/Annual Performance Report (SPP/APR).\n",
        "\n",
        "- **SPPI 15** - Resolution Sessions. % of hearing requests that went to resolution sessions that were resolved through resolution session settlement agreements.\n",
        "- **SPPI 16** -  Mediation. % of mediations held resulting in mediation agreements.\n",
        "\n",
        "> **LEAs -> CDRMS -> EDFacts -> EDPass -> SPP/APR -> SEA Determination**\n",
        "\n",
        "# **Python Scripts**\n"
      ],
      "metadata": {
        "id": "JP_8YaGTxSlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FS227 – Written, Signed Complaints**"
      ],
      "metadata": {
        "id": "cq4Onbet2pqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PROGRAM:  fs227_written_signed_complaints.py\n",
        "PURPOSE:  Import legacy SECTION/COUNT, create FS227 Header/Data tables via IF-THEN-DO,\n",
        "          then write the EDPass CSV (header record + data records; no column-name row).\n",
        "DATE:     2025-08-10\n",
        "AUTHOR:   Zane Wubbena, PhD\n",
        "CONTACT   zane.wubbena@tea.texas.gov\n",
        "INPUT:    CSV with columns: SECTION, COUNT\n",
        "OUTPUT:   UTF-8, CRLF CSV for EDPass (FS227 table: IDEABDISPRESWSC)\n",
        "\n",
        "RULES:\n",
        "  - Include all 7 required detail rows (1.1, 1.2, 1.3, 1.1(a), 1.1(b), 1.1(c), 1.2(a)).\n",
        "  - Use 0 for true zeros; use -1 for missing/unknown.\n",
        "  - EUT (Total): sum of 1.1 + 1.2 + 1.3 unless any is -1 → then EUT = -1.\n",
        "  - Exactly one WSC* category column populated per detail row; EUT has all category columns blank.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# =========================\n",
        "# Step 0: Configuration\n",
        "# =========================\n",
        "\n",
        "# Configuration parameters\n",
        "CONFIG = {\n",
        "    'OUT_FILE': 'txseaIDEABDWSCv000001.csv',  # must match header File Name (<=25 chars)\n",
        "    'FILE_ID': 'FS227_TX_FFY2024_SY2425_V001',  # File Identifier (<=32 chars)\n",
        "    'REPORT_PERIOD': '2024-2025',  # CCYY-CCYY\n",
        "    'STATE_CODE': 48,  # DG559 ANSI (2-digit, zero-padded)\n",
        "    'STATE_AGENCY': 1,  # DG570 (SEA=01)\n",
        "    'TABLE_NAME': 'IDEABDISPRESWSC',  # technical table name\n",
        "}\n",
        "\n",
        "# Explanations used in Data Records\n",
        "EXPLANATIONS = {\n",
        "    'STATUS_ISSUED': 'Written, Signed Complaints with reports issued as of 60 days following the end of the reporting period.',\n",
        "    'STATUS_PENDING': 'Written, Signed Complaints with reports pending as of 60 days following the end of the reporting period.',\n",
        "    'STATUS_WD_DISM': 'Written, Signed Complaints withdrawn or dismissed as of 60 days following the end of the reporting period.',\n",
        "    'FINDINGS_NC': 'Reports issued as of 60 days following the end of the reporting period with findings of noncompliance',\n",
        "    'TL_WITHIN': 'Reports issued as of 60 days following the end of the reporting period within the timelines (60 days)',\n",
        "    'TL_EXTENDED': 'Reports issued as of 60 days following the end of the reporting period within extended timelines',\n",
        "    'PEND_DPH': 'Written, Signed Complaints with reports pending as of 60 days following the end of the reporting period that are pending a due process hearing.',\n",
        "    'TOTAL': 'Total written, signed complaints.',\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# Step 0a: Guards\n",
        "# =========================\n",
        "\n",
        "def validate_config():\n",
        "    \"\"\"Validate configuration parameters meet length requirements.\"\"\"\n",
        "    if len(CONFIG['FILE_ID']) > 32:\n",
        "        raise ValueError(f\"FILE_ID exceeds 32 characters: {CONFIG['FILE_ID']}, length={len(CONFIG['FILE_ID'])} (max=32)\")\n",
        "\n",
        "    if len(CONFIG['OUT_FILE']) > 25:\n",
        "        raise ValueError(f\"OUT_FILE exceeds 25 characters: {CONFIG['OUT_FILE']}, length={len(CONFIG['OUT_FILE'])} (max=25)\")\n",
        "\n",
        "# =========================\n",
        "# Step 1: Create Test Data\n",
        "# =========================\n",
        "\n",
        "def create_test_data():\n",
        "    \"\"\"Create the test dataset shown in the image.\"\"\"\n",
        "    test_data = \"\"\"SECTION,COUNT\n",
        "Legacy 1.1,650\n",
        "Legacy 1.2,21\n",
        "Legacy 1.3,522\n",
        "Legacy 1.1(a),386\n",
        "Legacy 1.1(b),629\n",
        "Legacy 1.1(c),21\n",
        "Legacy 1.2(a),20\"\"\"\n",
        "\n",
        "    return pd.read_csv(StringIO(test_data))\n",
        "\n",
        "# =========================\n",
        "# Step 2: Process Data\n",
        "# =========================\n",
        "\n",
        "def normalize_data(df):\n",
        "    \"\"\"Normalize SECTION text and coerce COUNT to numeric; enforce -1 for missing/negative.\"\"\"\n",
        "    df = df.copy()\n",
        "    df['SECTION'] = df['SECTION'].str.strip()\n",
        "\n",
        "    # Coerce COUNT to numeric\n",
        "    df['COUNT'] = pd.to_numeric(df['COUNT'], errors='coerce')\n",
        "\n",
        "    # Replace missing or negative values with -1\n",
        "    df.loc[df['COUNT'].isna() | (df['COUNT'] < 0), 'COUNT'] = -1\n",
        "\n",
        "    # Round to integers\n",
        "    df['COUNT'] = df['COUNT'].round(0).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "def build_data_records(df):\n",
        "    \"\"\"Build data records (detail) with IF-THEN-DO logic.\"\"\"\n",
        "    records = []\n",
        "\n",
        "    # Track which required sections appeared\n",
        "    found_sections = {\n",
        "        '1.1': False,\n",
        "        '1.2': False,\n",
        "        '1.3': False,\n",
        "        '1.1(a)': False,\n",
        "        '1.1(b)': False,\n",
        "        '1.1(c)': False,\n",
        "        '1.2(a)': False\n",
        "    }\n",
        "\n",
        "    # Track sum/missing for EUT\n",
        "    sum_status = 0\n",
        "    any_missing = False\n",
        "\n",
        "    # Process each row in the input data\n",
        "    for _, row in df.iterrows():\n",
        "        section = row['SECTION']\n",
        "        count = row['COUNT']\n",
        "\n",
        "        record = {\n",
        "            'DG559_State_Code': f\"{CONFIG['STATE_CODE']:02d}\",\n",
        "            'DG570_State_Agency_Number': f\"{CONFIG['STATE_AGENCY']:02d}\",\n",
        "            'Filler1': '',\n",
        "            'Filler2': '',\n",
        "            'Table_Name': CONFIG['TABLE_NAME'],\n",
        "            'Filler3': '',\n",
        "            'WSC_Status': '',\n",
        "            'WSC_Findings': '',\n",
        "            'WSC_Timelines': '',\n",
        "            'WSC_Pending_Type': '',\n",
        "            'Total_Indicator': 'N',\n",
        "            'Explanation': '',\n",
        "            'WSC_Count': count\n",
        "        }\n",
        "\n",
        "        # Map SECTION → category/value\n",
        "        if section == 'Legacy 1.1':\n",
        "            found_sections['1.1'] = True\n",
        "            record['WSC_Status'] = 'CMPLNTSRPTISSD'\n",
        "            record['Explanation'] = EXPLANATIONS['STATUS_ISSUED']\n",
        "            if count == -1:\n",
        "                any_missing = True\n",
        "            else:\n",
        "                sum_status += count\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'Legacy 1.2':\n",
        "            found_sections['1.2'] = True\n",
        "            record['WSC_Status'] = 'CMPLNTSPNDG'\n",
        "            record['Explanation'] = EXPLANATIONS['STATUS_PENDING']\n",
        "            if count == -1:\n",
        "                any_missing = True\n",
        "            else:\n",
        "                sum_status += count\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'Legacy 1.3':\n",
        "            found_sections['1.3'] = True\n",
        "            record['WSC_Status'] = 'CMPLNTSWDRNDIS'\n",
        "            record['Explanation'] = EXPLANATIONS['STATUS_WD_DISM']\n",
        "            if count == -1:\n",
        "                any_missing = True\n",
        "            else:\n",
        "                sum_status += count\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'Legacy 1.1(a)':\n",
        "            found_sections['1.1(a)'] = True\n",
        "            record['WSC_Findings'] = 'RPTSWFNDGSNC'\n",
        "            record['Explanation'] = EXPLANATIONS['FINDINGS_NC']\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'Legacy 1.1(b)':\n",
        "            found_sections['1.1(b)'] = True\n",
        "            record['WSC_Timelines'] = 'RPTSWTHNTMLNS'\n",
        "            record['Explanation'] = EXPLANATIONS['TL_WITHIN']\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'Legacy 1.1(c)':\n",
        "            found_sections['1.1(c)'] = True\n",
        "            record['WSC_Timelines'] = 'RPTSWTHNXTMLNS'\n",
        "            record['Explanation'] = EXPLANATIONS['TL_EXTENDED']\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'Legacy 1.2(a)':\n",
        "            found_sections['1.2(a)'] = True\n",
        "            record['WSC_Pending_Type'] = 'CMPLNTSPNDGDPH'\n",
        "            record['Explanation'] = EXPLANATIONS['PEND_DPH']\n",
        "            records.append(record)\n",
        "\n",
        "    # Add any missing required sections with WSC_Count = -1\n",
        "    missing_mappings = {\n",
        "        '1.1': ('WSC_Status', 'CMPLNTSRPTISSD', 'STATUS_ISSUED'),\n",
        "        '1.2': ('WSC_Status', 'CMPLNTSPNDG', 'STATUS_PENDING'),\n",
        "        '1.3': ('WSC_Status', 'CMPLNTSWDRNDIS', 'STATUS_WD_DISM'),\n",
        "        '1.1(a)': ('WSC_Findings', 'RPTSWFNDGSNC', 'FINDINGS_NC'),\n",
        "        '1.1(b)': ('WSC_Timelines', 'RPTSWTHNTMLNS', 'TL_WITHIN'),\n",
        "        '1.1(c)': ('WSC_Timelines', 'RPTSWTHNXTMLNS', 'TL_EXTENDED'),\n",
        "        '1.2(a)': ('WSC_Pending_Type', 'CMPLNTSPNDGDPH', 'PEND_DPH')\n",
        "    }\n",
        "\n",
        "    for section_key, found in found_sections.items():\n",
        "        if not found:\n",
        "            if section_key in ['1.1', '1.2', '1.3']:\n",
        "                any_missing = True\n",
        "\n",
        "            cat_field, cat_value, expl_key = missing_mappings[section_key]\n",
        "            record = {\n",
        "                'DG559_State_Code': f\"{CONFIG['STATE_CODE']:02d}\",\n",
        "                'DG570_State_Agency_Number': f\"{CONFIG['STATE_AGENCY']:02d}\",\n",
        "                'Filler1': '',\n",
        "                'Filler2': '',\n",
        "                'Table_Name': CONFIG['TABLE_NAME'],\n",
        "                'Filler3': '',\n",
        "                'WSC_Status': '',\n",
        "                'WSC_Findings': '',\n",
        "                'WSC_Timelines': '',\n",
        "                'WSC_Pending_Type': '',\n",
        "                'Total_Indicator': 'N',\n",
        "                'Explanation': EXPLANATIONS[expl_key],\n",
        "                'WSC_Count': -1\n",
        "            }\n",
        "            record[cat_field] = cat_value\n",
        "            records.append(record)\n",
        "\n",
        "    # Add Education Unit Total (EUT) row\n",
        "    eut_record = {\n",
        "        'DG559_State_Code': f\"{CONFIG['STATE_CODE']:02d}\",\n",
        "        'DG570_State_Agency_Number': f\"{CONFIG['STATE_AGENCY']:02d}\",\n",
        "        'Filler1': '',\n",
        "        'Filler2': '',\n",
        "        'Table_Name': CONFIG['TABLE_NAME'],\n",
        "        'Filler3': '',\n",
        "        'WSC_Status': '',\n",
        "        'WSC_Findings': '',\n",
        "        'WSC_Timelines': '',\n",
        "        'WSC_Pending_Type': '',\n",
        "        'Total_Indicator': 'Y',\n",
        "        'Explanation': EXPLANATIONS['TOTAL'],\n",
        "        'WSC_Count': -1 if any_missing else sum_status\n",
        "    }\n",
        "    records.append(eut_record)\n",
        "\n",
        "    # Create DataFrame and add File_Record_Number\n",
        "    df_records = pd.DataFrame(records)\n",
        "    df_records.insert(0, 'File_Record_Number', range(1, len(df_records) + 1))\n",
        "\n",
        "    return df_records\n",
        "\n",
        "def create_header_record(n_records):\n",
        "    \"\"\"Create the header record (1 row).\"\"\"\n",
        "    header = {\n",
        "        'File_Type': 'SEA PART B DISPUTE COMPLAINTS',\n",
        "        'Total_Records_in_File': n_records,\n",
        "        'File_Name': CONFIG['OUT_FILE'],\n",
        "        'File_Identifier': CONFIG['FILE_ID'],\n",
        "        'File_Reporting_Period': CONFIG['REPORT_PERIOD'],\n",
        "        'Filler': ''\n",
        "    }\n",
        "    return pd.DataFrame([header])\n",
        "\n",
        "def write_csv(header_df, data_df, filename):\n",
        "    \"\"\"Write final CSV (values only; no column headers).\"\"\"\n",
        "    # Write header row\n",
        "    header_csv = header_df.to_csv(index=False, header=False, lineterminator='\\r\\n')\n",
        "\n",
        "    # Write data rows\n",
        "    data_csv = data_df.to_csv(index=False, header=False, lineterminator='\\r\\n')\n",
        "\n",
        "    # Combine\n",
        "    final_csv = header_csv + data_csv\n",
        "\n",
        "    # Save to file\n",
        "    with open(filename, 'w', encoding='utf-8', newline='') as f:\n",
        "        f.write(final_csv)\n",
        "\n",
        "    return final_csv\n",
        "\n",
        "# =========================\n",
        "# Main Execution\n",
        "# =========================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "    print(\"FS227 IF-THEN Build Script\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Validate configuration\n",
        "    validate_config()\n",
        "    print(\"✓ Configuration validated\")\n",
        "\n",
        "    # Option 1: Use test data (comment out if uploading file)\n",
        "    print(\"\\nUsing test data from the example...\")\n",
        "    df_input = create_test_data()\n",
        "\n",
        "    # Option 2: Upload file (uncomment to use)\n",
        "    # print(\"\\nPlease upload your input CSV file...\")\n",
        "    # uploaded = files.upload()\n",
        "    # filename = list(uploaded.keys())[0]\n",
        "    # df_input = pd.read_csv(filename)\n",
        "\n",
        "    print(f\"\\nInput data shape: {df_input.shape}\")\n",
        "    print(\"\\nInput data preview:\")\n",
        "    print(df_input)\n",
        "\n",
        "    # Step 1: Normalize data\n",
        "    df_normalized = normalize_data(df_input)\n",
        "    print(\"\\n✓ Data normalized\")\n",
        "\n",
        "    # Step 2: Build data records\n",
        "    df_data = build_data_records(df_normalized)\n",
        "    print(f\"✓ Data records built: {len(df_data)} rows\")\n",
        "\n",
        "    # Step 3: Create header record\n",
        "    df_header = create_header_record(len(df_data))\n",
        "    print(\"✓ Header record created\")\n",
        "\n",
        "    # Step 4: Write CSV\n",
        "    output_filename = CONFIG['OUT_FILE']\n",
        "    write_csv(df_header, df_data, output_filename)\n",
        "    print(f\"\\n✓ CSV file written: {output_filename}\")\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"HEADER RECORD:\")\n",
        "    print(df_header.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"DATA RECORDS:\")\n",
        "    print(df_data.to_string(index=False))\n",
        "\n",
        "    # QA Summary\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"QA SUMMARY:\")\n",
        "    print(f\"Total records: {len(df_data)}\")\n",
        "    print(f\"Detail records (Total_Indicator='N'): {len(df_data[df_data['Total_Indicator'] == 'N'])}\")\n",
        "    print(f\"EUT record (Total_Indicator='Y'): {len(df_data[df_data['Total_Indicator'] == 'Y'])}\")\n",
        "\n",
        "    # Download the file\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Downloading output file...\")\n",
        "    files.download(output_filename)\n",
        "\n",
        "    return df_header, df_data\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    header_df, data_df = main()\n",
        "\n",
        "# =========================\n",
        "#  Python Program End\n",
        "# ==========================="
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        },
        "id": "jlrsQUq_2u7m",
        "outputId": "4ad8e080-dc8a-4401-f9b9-76df62cbbf8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FS227 IF-THEN Build Script\n",
            "==================================================\n",
            "✓ Configuration validated\n",
            "\n",
            "Using test data from the example...\n",
            "\n",
            "Input data shape: (7, 2)\n",
            "\n",
            "Input data preview:\n",
            "         SECTION  COUNT\n",
            "0     Legacy 1.1    650\n",
            "1     Legacy 1.2     21\n",
            "2     Legacy 1.3    522\n",
            "3  Legacy 1.1(a)    386\n",
            "4  Legacy 1.1(b)    629\n",
            "5  Legacy 1.1(c)     21\n",
            "6  Legacy 1.2(a)     20\n",
            "\n",
            "✓ Data normalized\n",
            "✓ Data records built: 8 rows\n",
            "✓ Header record created\n",
            "\n",
            "✓ CSV file written: txseaIDEABDWSCv000001.csv\n",
            "\n",
            "==================================================\n",
            "HEADER RECORD:\n",
            "                    File_Type  Total_Records_in_File                 File_Name              File_Identifier File_Reporting_Period Filler\n",
            "SEA PART B DISPUTE COMPLAINTS                      8 txseaIDEABDWSCv000001.csv FS227_TX_FFY2024_SY2425_V001             2024-2025       \n",
            "\n",
            "==================================================\n",
            "DATA RECORDS:\n",
            " File_Record_Number DG559_State_Code DG570_State_Agency_Number Filler1 Filler2      Table_Name Filler3     WSC_Status WSC_Findings  WSC_Timelines WSC_Pending_Type Total_Indicator                                                                                                                                     Explanation  WSC_Count\n",
            "                  1               48                        01                 IDEABDISPRESWSC         CMPLNTSRPTISSD                                                            N                                         Written, Signed Complaints with reports issued as of 60 days following the end of the reporting period.        650\n",
            "                  2               48                        01                 IDEABDISPRESWSC            CMPLNTSPNDG                                                            N                                        Written, Signed Complaints with reports pending as of 60 days following the end of the reporting period.         21\n",
            "                  3               48                        01                 IDEABDISPRESWSC         CMPLNTSWDRNDIS                                                            N                                      Written, Signed Complaints withdrawn or dismissed as of 60 days following the end of the reporting period.        522\n",
            "                  4               48                        01                 IDEABDISPRESWSC                        RPTSWFNDGSNC                                               N                                           Reports issued as of 60 days following the end of the reporting period with findings of noncompliance        386\n",
            "                  5               48                        01                 IDEABDISPRESWSC                                      RPTSWTHNTMLNS                                N                                           Reports issued as of 60 days following the end of the reporting period within the timelines (60 days)        629\n",
            "                  6               48                        01                 IDEABDISPRESWSC                                     RPTSWTHNXTMLNS                                N                                                Reports issued as of 60 days following the end of the reporting period within extended timelines         21\n",
            "                  7               48                        01                 IDEABDISPRESWSC                                                      CMPLNTSPNDGDPH               N Written, Signed Complaints with reports pending as of 60 days following the end of the reporting period that are pending a due process hearing.         20\n",
            "                  8               48                        01                 IDEABDISPRESWSC                                                                                   Y                                                                                                               Total written, signed complaints.       1193\n",
            "\n",
            "==================================================\n",
            "QA SUMMARY:\n",
            "Total records: 8\n",
            "Detail records (Total_Indicator='N'): 7\n",
            "EUT record (Total_Indicator='Y'): 1\n",
            "\n",
            "==================================================\n",
            "Downloading output file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bc9c9b61-a7bc-4acb-bb6e-8d3a49bff4d8\", \"txseaIDEABDWSCv000001.csv\", 1298)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FS228 – Mediation Requests**"
      ],
      "metadata": {
        "id": "3q1d8Sky21lN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PROGRAM:  fs228_mediation_requests.py\n",
        "PURPOSE:  Import legacy SECTION/COUNT, create FS228 Header/Data tables via IF-THEN-DO,\n",
        "          then write the EDPass CSV (header record + data records; no column-name row).\n",
        "DATE:     2025-08-10\n",
        "AUTHOR:   Zane Wubbena, PhD\n",
        "CONTACT   zane.wubbena@tea.texas.gov\n",
        "INPUT:    CSV with columns: SECTION, COUNT\n",
        "OUTPUT:   UTF-8, CRLF CSV for EDPass (FS228 table: IDEABDISPRESMEDREQ)\n",
        "\n",
        "RULES:\n",
        "  - Required detail rows (7 total):\n",
        "      * Status (3): 2.1 -> MEDTNHLD; 2.2 -> MEDTNPND; 2.3 -> MEDTNWTHDRW\n",
        "      * Type   (2): 2.1(a) -> MEDTNHLDDPH; 2.1(b) -> MEDTNHLDNODPH\n",
        "      * Type×Outcome (2): 2.1(a)(i) -> MEDTNHLDDPH + MEDTNAGRMNT\n",
        "                          2.1(b)(i) -> MEDTNHLDNODPH + MEDTNAGRMNT\n",
        "  - Use 0 for true zeros; use -1 for missing/unknown.\n",
        "  - EUT (Total): sum of MEDTNHLD + MEDTNPND + MEDTNWTHDRW unless any is -1 → then EUT = -1.\n",
        "  - Exactly one category column populated per detail row (except the Type×Outcome rows, which have both Type and Outcome).\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# =========================\n",
        "# Step 0: Configuration\n",
        "# =========================\n",
        "\n",
        "# Configuration parameters\n",
        "CONFIG = {\n",
        "    'OUT_FILE': 'txseaIDEABDMRv000001.csv',  # must match header File Name (<=25 chars)\n",
        "    'FILE_ID': 'FS228_TX_FFY2024_SY2425_V001',  # File Identifier (<=32 chars)\n",
        "    'REPORT_PERIOD': '2024-2025',  # CCYY-CCYY\n",
        "    'STATE_CODE': 48,  # DG559 ANSI (2-digit, zero-padded)\n",
        "    'STATE_AGENCY': 1,  # DG570 (SEA=01)\n",
        "    'TABLE_NAME': 'IDEABDISPRESMEDREQ',  # FS228 technical table name\n",
        "}\n",
        "\n",
        "# Explanations used in Data Records\n",
        "EXPLANATIONS = {\n",
        "    'STATUS_HELD': 'Mediation requests resulting in mediations held as of the end of the reporting period.',\n",
        "    'STATUS_PEND': 'Mediations pending as of the end of the reporting period (including mediation requests pending).',\n",
        "    'STATUS_WDNH': 'Mediation requests withdrawn or not held as of the end of the reporting period.',\n",
        "    'TYPE_DPH': 'Mediations held that were related to due process complaints.',\n",
        "    'TYPE_NODPH': 'Mediations held that were not related to due process complaints.',\n",
        "    'AGRMNT_DPH': 'Mediations held related to due process complaints that ended with a mediation agreement.',\n",
        "    'AGRMNT_NODPH': 'Mediations held not related to due process complaints that ended with a mediation agreement.',\n",
        "    'TOTAL': 'Total mediation requests.',\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# Step 0a: Guards\n",
        "# =========================\n",
        "\n",
        "def validate_config():\n",
        "    \"\"\"Validate configuration parameters meet length requirements.\"\"\"\n",
        "    if len(CONFIG['FILE_ID']) > 32:\n",
        "        raise ValueError(f\"FILE_ID exceeds 32 characters: {CONFIG['FILE_ID']}, length={len(CONFIG['FILE_ID'])} (max=32)\")\n",
        "\n",
        "    if len(CONFIG['OUT_FILE']) > 25:\n",
        "        raise ValueError(f\"OUT_FILE exceeds 25 characters: {CONFIG['OUT_FILE']}, length={len(CONFIG['OUT_FILE'])} (max=25)\")\n",
        "\n",
        "# =========================\n",
        "# Step 1: Create Test Data\n",
        "# =========================\n",
        "\n",
        "def create_test_data():\n",
        "    \"\"\"Create comprehensive test dataset for FS228 with various scenarios.\"\"\"\n",
        "    test_data = \"\"\"SECTION,COUNT\n",
        "Legacy 2.1,258\n",
        "Legacy 2.2,121\n",
        "Legacy 2.3,50\n",
        "Legacy 2.1(a),168\n",
        "Legacy 2.1(b),90\n",
        "Legacy 2.1(a)(i),112\n",
        "Legacy 2.1(b)(i),56\"\"\"\n",
        "\n",
        "    return pd.read_csv(StringIO(test_data))\n",
        "\n",
        "def create_test_data_with_missing():\n",
        "    \"\"\"Create test dataset with missing sections to test gap-filling logic.\"\"\"\n",
        "    test_data = \"\"\"SECTION,COUNT\n",
        "Legacy 2.1,258\n",
        "Legacy 2.3,50\n",
        "Legacy 2.1(b),90\n",
        "Legacy 2.1(a)(i),112\"\"\"\n",
        "\n",
        "    return pd.read_csv(StringIO(test_data))\n",
        "\n",
        "def create_test_data_with_negatives():\n",
        "    \"\"\"Create test dataset with negative/missing values to test -1 handling.\"\"\"\n",
        "    test_data = \"\"\"SECTION,COUNT\n",
        "Legacy 2.1,-5\n",
        "Legacy 2.2,\n",
        "Legacy 2.3,50\n",
        "Legacy 2.1(a),168\n",
        "Legacy 2.1(b),90\n",
        "Legacy 2.1(a)(i),112\n",
        "Legacy 2.1(b)(i),\"\"\"\n",
        "\n",
        "    return pd.read_csv(StringIO(test_data))\n",
        "\n",
        "# =========================\n",
        "# Step 2: Process Data\n",
        "# =========================\n",
        "\n",
        "def normalize_data(df):\n",
        "    \"\"\"\n",
        "    Normalize SECTION text and coerce COUNT to numeric.\n",
        "    Aggregate duplicates by SECTION (sum).\n",
        "    Enforce -1 for missing/negative values.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Normalize SECTION to uppercase and strip whitespace\n",
        "    df['SECTION'] = df['SECTION'].str.strip().str.upper()\n",
        "\n",
        "    # Coerce COUNT to numeric\n",
        "    df['COUNT'] = pd.to_numeric(df['COUNT'], errors='coerce')\n",
        "\n",
        "    # Aggregate by SECTION (sum duplicates)\n",
        "    df = df.groupby('SECTION', as_index=False)['COUNT'].sum()\n",
        "\n",
        "    # Replace missing or negative values with -1\n",
        "    df.loc[df['COUNT'].isna() | (df['COUNT'] < 0), 'COUNT'] = -1\n",
        "\n",
        "    # Round to integers\n",
        "    df['COUNT'] = df['COUNT'].round(0).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "def build_data_records(df):\n",
        "    \"\"\"Build data records (detail) with IF-THEN-DO logic for FS228.\"\"\"\n",
        "    records = []\n",
        "\n",
        "    # Track which required sections appeared\n",
        "    found_sections = {\n",
        "        'stat_21': False,   # Legacy 2.1\n",
        "        'stat_22': False,   # Legacy 2.2\n",
        "        'stat_23': False,   # Legacy 2.3\n",
        "        'typ_21a': False,   # Legacy 2.1(a)\n",
        "        'typ_21b': False,   # Legacy 2.1(b)\n",
        "        'out_21ai': False,  # Legacy 2.1(a)(i)\n",
        "        'out_21bi': False,  # Legacy 2.1(b)(i)\n",
        "    }\n",
        "\n",
        "    # Track sum/missing for EUT (only Status rows contribute to total)\n",
        "    sum_status = 0\n",
        "    any_missing = False\n",
        "\n",
        "    # Process each row in the input data\n",
        "    for _, row in df.iterrows():\n",
        "        section = row['SECTION']\n",
        "        count = row['COUNT']\n",
        "\n",
        "        # Base record structure\n",
        "        record = {\n",
        "            'DG559_State_Code': f\"{CONFIG['STATE_CODE']:02d}\",\n",
        "            'DG570_State_Agency_Number': f\"{CONFIG['STATE_AGENCY']:02d}\",\n",
        "            'Filler1': '',\n",
        "            'Filler2': '',\n",
        "            'Table_Name': CONFIG['TABLE_NAME'],\n",
        "            'Filler3': '',\n",
        "            'MR_Status': '',\n",
        "            'MR_Type': '',\n",
        "            'MR_Outcome': '',\n",
        "            'Total_Indicator': 'N',\n",
        "            'Explanation': '',\n",
        "            'MR_Count': count\n",
        "        }\n",
        "\n",
        "        # Map SECTION → category/value based on SAS logic\n",
        "        if section == 'LEGACY 2.1':\n",
        "            found_sections['stat_21'] = True\n",
        "            record['MR_Status'] = 'MEDTNHLD'\n",
        "            record['Explanation'] = EXPLANATIONS['STATUS_HELD']\n",
        "            if count == -1:\n",
        "                any_missing = True\n",
        "            else:\n",
        "                sum_status += count\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'LEGACY 2.2':\n",
        "            found_sections['stat_22'] = True\n",
        "            record['MR_Status'] = 'MEDTNPND'\n",
        "            record['Explanation'] = EXPLANATIONS['STATUS_PEND']\n",
        "            if count == -1:\n",
        "                any_missing = True\n",
        "            else:\n",
        "                sum_status += count\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'LEGACY 2.3':\n",
        "            found_sections['stat_23'] = True\n",
        "            record['MR_Status'] = 'MEDTNWTHDRW'\n",
        "            record['Explanation'] = EXPLANATIONS['STATUS_WDNH']\n",
        "            if count == -1:\n",
        "                any_missing = True\n",
        "            else:\n",
        "                sum_status += count\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'LEGACY 2.1(A)':\n",
        "            found_sections['typ_21a'] = True\n",
        "            record['MR_Type'] = 'MEDTNHLDDPH'\n",
        "            record['Explanation'] = EXPLANATIONS['TYPE_DPH']\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'LEGACY 2.1(B)':\n",
        "            found_sections['typ_21b'] = True\n",
        "            record['MR_Type'] = 'MEDTNHLDNODPH'\n",
        "            record['Explanation'] = EXPLANATIONS['TYPE_NODPH']\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'LEGACY 2.1(A)(I)':\n",
        "            found_sections['out_21ai'] = True\n",
        "            record['MR_Type'] = 'MEDTNHLDDPH'\n",
        "            record['MR_Outcome'] = 'MEDTNAGRMNT'\n",
        "            record['Explanation'] = EXPLANATIONS['AGRMNT_DPH']\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'LEGACY 2.1(B)(I)':\n",
        "            found_sections['out_21bi'] = True\n",
        "            record['MR_Type'] = 'MEDTNHLDNODPH'\n",
        "            record['MR_Outcome'] = 'MEDTNAGRMNT'\n",
        "            record['Explanation'] = EXPLANATIONS['AGRMNT_NODPH']\n",
        "            records.append(record)\n",
        "\n",
        "        # Otherwise ignore unmapped sections (could log if desired)\n",
        "\n",
        "    # Add any missing required sections with MR_Count = -1\n",
        "\n",
        "    # Status rows (3) - these affect the EUT calculation\n",
        "    if not found_sections['stat_21']:\n",
        "        any_missing = True\n",
        "        record = create_base_record()\n",
        "        record['MR_Status'] = 'MEDTNHLD'\n",
        "        record['Explanation'] = EXPLANATIONS['STATUS_HELD']\n",
        "        record['MR_Count'] = -1\n",
        "        records.append(record)\n",
        "\n",
        "    if not found_sections['stat_22']:\n",
        "        any_missing = True\n",
        "        record = create_base_record()\n",
        "        record['MR_Status'] = 'MEDTNPND'\n",
        "        record['Explanation'] = EXPLANATIONS['STATUS_PEND']\n",
        "        record['MR_Count'] = -1\n",
        "        records.append(record)\n",
        "\n",
        "    if not found_sections['stat_23']:\n",
        "        any_missing = True\n",
        "        record = create_base_record()\n",
        "        record['MR_Status'] = 'MEDTNWTHDRW'\n",
        "        record['Explanation'] = EXPLANATIONS['STATUS_WDNH']\n",
        "        record['MR_Count'] = -1\n",
        "        records.append(record)\n",
        "\n",
        "    # Type rows (2) - these don't affect the EUT calculation\n",
        "    if not found_sections['typ_21a']:\n",
        "        record = create_base_record()\n",
        "        record['MR_Type'] = 'MEDTNHLDDPH'\n",
        "        record['Explanation'] = EXPLANATIONS['TYPE_DPH']\n",
        "        record['MR_Count'] = -1\n",
        "        records.append(record)\n",
        "\n",
        "    if not found_sections['typ_21b']:\n",
        "        record = create_base_record()\n",
        "        record['MR_Type'] = 'MEDTNHLDNODPH'\n",
        "        record['Explanation'] = EXPLANATIONS['TYPE_NODPH']\n",
        "        record['MR_Count'] = -1\n",
        "        records.append(record)\n",
        "\n",
        "    # Type×Outcome rows (2) - both Type and Outcome populated\n",
        "    if not found_sections['out_21ai']:\n",
        "        record = create_base_record()\n",
        "        record['MR_Type'] = 'MEDTNHLDDPH'\n",
        "        record['MR_Outcome'] = 'MEDTNAGRMNT'\n",
        "        record['Explanation'] = EXPLANATIONS['AGRMNT_DPH']\n",
        "        record['MR_Count'] = -1\n",
        "        records.append(record)\n",
        "\n",
        "    if not found_sections['out_21bi']:\n",
        "        record = create_base_record()\n",
        "        record['MR_Type'] = 'MEDTNHLDNODPH'\n",
        "        record['MR_Outcome'] = 'MEDTNAGRMNT'\n",
        "        record['Explanation'] = EXPLANATIONS['AGRMNT_NODPH']\n",
        "        record['MR_Count'] = -1\n",
        "        records.append(record)\n",
        "\n",
        "    # Add Education Unit Total (EUT) row\n",
        "    eut_record = create_base_record()\n",
        "    eut_record['Total_Indicator'] = 'Y'\n",
        "    eut_record['Explanation'] = EXPLANATIONS['TOTAL']\n",
        "    eut_record['MR_Count'] = -1 if any_missing else sum_status\n",
        "    records.append(eut_record)\n",
        "\n",
        "    # Create DataFrame and add File_Record_Number\n",
        "    df_records = pd.DataFrame(records)\n",
        "    df_records.insert(0, 'File_Record_Number', range(1, len(df_records) + 1))\n",
        "\n",
        "    return df_records\n",
        "\n",
        "def create_base_record():\n",
        "    \"\"\"Create a base record with all fields initialized.\"\"\"\n",
        "    return {\n",
        "        'DG559_State_Code': f\"{CONFIG['STATE_CODE']:02d}\",\n",
        "        'DG570_State_Agency_Number': f\"{CONFIG['STATE_AGENCY']:02d}\",\n",
        "        'Filler1': '',\n",
        "        'Filler2': '',\n",
        "        'Table_Name': CONFIG['TABLE_NAME'],\n",
        "        'Filler3': '',\n",
        "        'MR_Status': '',\n",
        "        'MR_Type': '',\n",
        "        'MR_Outcome': '',\n",
        "        'Total_Indicator': 'N',\n",
        "        'Explanation': '',\n",
        "        'MR_Count': 0\n",
        "    }\n",
        "\n",
        "def create_header_record(n_records):\n",
        "    \"\"\"Create the header record (1 row) for FS228.\"\"\"\n",
        "    header = {\n",
        "        'File_Type': 'SEA PART B MED REQ',  # FS228 specific header\n",
        "        'Total_Records_in_File': n_records,\n",
        "        'File_Name': CONFIG['OUT_FILE'],\n",
        "        'File_Identifier': CONFIG['FILE_ID'],\n",
        "        'File_Reporting_Period': CONFIG['REPORT_PERIOD'],\n",
        "        'Filler': ''\n",
        "    }\n",
        "    return pd.DataFrame([header])\n",
        "\n",
        "def write_csv(header_df, data_df, filename):\n",
        "    \"\"\"Write final CSV (values only; no column headers).\"\"\"\n",
        "    # Write header row\n",
        "    header_csv = header_df.to_csv(index=False, header=False, lineterminator='\\r\\n')\n",
        "\n",
        "    # Write data rows\n",
        "    data_csv = data_df.to_csv(index=False, header=False, lineterminator='\\r\\n')\n",
        "\n",
        "    # Combine\n",
        "    final_csv = header_csv + data_csv\n",
        "\n",
        "    # Save to file\n",
        "    with open(filename, 'w', encoding='utf-8', newline='') as f:\n",
        "        f.write(final_csv)\n",
        "\n",
        "    return final_csv\n",
        "\n",
        "# =========================\n",
        "# Main Execution\n",
        "# =========================\n",
        "\n",
        "def main(test_scenario='complete'):\n",
        "    \"\"\"\n",
        "    Main execution function.\n",
        "\n",
        "    Args:\n",
        "        test_scenario: Which test dataset to use\n",
        "            - 'complete': All sections present with valid data\n",
        "            - 'missing': Some sections missing to test gap-filling\n",
        "            - 'negatives': Test handling of negative/missing values\n",
        "            - 'upload': Upload your own file\n",
        "    \"\"\"\n",
        "    print(\"FS228 Mediation Requests Build Script\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Validate configuration\n",
        "    validate_config()\n",
        "    print(\"✓ Configuration validated\")\n",
        "\n",
        "    # Select test data based on scenario\n",
        "    if test_scenario == 'complete':\n",
        "        print(\"\\nUsing COMPLETE test data (all sections present)...\")\n",
        "        df_input = create_test_data()\n",
        "    elif test_scenario == 'missing':\n",
        "        print(\"\\nUsing test data with MISSING sections...\")\n",
        "        df_input = create_test_data_with_missing()\n",
        "    elif test_scenario == 'negatives':\n",
        "        print(\"\\nUsing test data with NEGATIVE/MISSING values...\")\n",
        "        df_input = create_test_data_with_negatives()\n",
        "    elif test_scenario == 'upload':\n",
        "        print(\"\\nPlease upload your input CSV file...\")\n",
        "        uploaded = files.upload()\n",
        "        filename = list(uploaded.keys())[0]\n",
        "        df_input = pd.read_csv(filename)\n",
        "    else:\n",
        "        print(f\"\\nUnknown scenario '{test_scenario}'. Using complete test data...\")\n",
        "        df_input = create_test_data()\n",
        "\n",
        "    print(f\"\\nInput data shape: {df_input.shape}\")\n",
        "    print(\"\\nInput data preview:\")\n",
        "    print(df_input)\n",
        "\n",
        "    # Step 1: Normalize data\n",
        "    df_normalized = normalize_data(df_input)\n",
        "    print(\"\\n✓ Data normalized\")\n",
        "    print(\"\\nNormalized data:\")\n",
        "    print(df_normalized)\n",
        "\n",
        "    # Step 2: Build data records\n",
        "    df_data = build_data_records(df_normalized)\n",
        "    print(f\"\\n✓ Data records built: {len(df_data)} rows\")\n",
        "\n",
        "    # Step 3: Create header record\n",
        "    df_header = create_header_record(len(df_data))\n",
        "    print(\"✓ Header record created\")\n",
        "\n",
        "    # Step 4: Write CSV\n",
        "    output_filename = CONFIG['OUT_FILE']\n",
        "    write_csv(df_header, df_data, output_filename)\n",
        "    print(f\"\\n✓ CSV file written: {output_filename}\")\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"HEADER RECORD:\")\n",
        "    print(df_header.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"DATA RECORDS:\")\n",
        "    print(df_data.to_string(index=False))\n",
        "\n",
        "    # QA Summary\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"QA SUMMARY:\")\n",
        "    print(f\"Total records: {len(df_data)}\")\n",
        "    print(f\"Detail records (Total_Indicator='N'): {len(df_data[df_data['Total_Indicator'] == 'N'])}\")\n",
        "    print(f\"EUT record (Total_Indicator='Y'): {len(df_data[df_data['Total_Indicator'] == 'Y'])}\")\n",
        "\n",
        "    # Category distribution\n",
        "    print(\"\\nCategory Distribution:\")\n",
        "    print(f\"MR_Status populated: {len(df_data[df_data['MR_Status'] != ''])} rows\")\n",
        "    print(f\"MR_Type populated: {len(df_data[df_data['MR_Type'] != ''])} rows\")\n",
        "    print(f\"MR_Outcome populated: {len(df_data[df_data['MR_Outcome'] != ''])} rows\")\n",
        "\n",
        "    # Value distribution\n",
        "    print(\"\\nMR_Count Distribution:\")\n",
        "    print(df_data['MR_Count'].value_counts().sort_index())\n",
        "\n",
        "    # Download the file\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Downloading output file...\")\n",
        "    files.download(output_filename)\n",
        "\n",
        "    return df_header, df_data\n",
        "\n",
        "# Run the main function with different test scenarios\n",
        "if __name__ == \"__main__\":\n",
        "    # Choose which test scenario to run:\n",
        "    # 'complete' - all sections with valid data\n",
        "    # 'missing' - some sections missing\n",
        "    # 'negatives' - test -1 handling\n",
        "    # 'upload' - upload your own file\n",
        "\n",
        "    header_df, data_df = main(test_scenario='complete')\n",
        "\n",
        "    # Uncomment to test other scenarios:\n",
        "    # header_df, data_df = main(test_scenario='missing')\n",
        "    # header_df, data_df = main(test_scenario='negatives')\n",
        "    # header_df, data_df = main(test_scenario='upload')\n",
        "\n",
        "# =========================\n",
        "#  Python Program End\n",
        "# =========================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ggAKPSxk25kR",
        "outputId": "3bd07fa6-1e4e-4690-d06c-6eda4e3a07eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FS228 Mediation Requests Build Script\n",
            "==================================================\n",
            "✓ Configuration validated\n",
            "\n",
            "Using COMPLETE test data (all sections present)...\n",
            "\n",
            "Input data shape: (7, 2)\n",
            "\n",
            "Input data preview:\n",
            "            SECTION  COUNT\n",
            "0        Legacy 2.1     25\n",
            "1        Legacy 2.2      8\n",
            "2        Legacy 2.3      3\n",
            "3     Legacy 2.1(a)     12\n",
            "4     Legacy 2.1(b)     13\n",
            "5  Legacy 2.1(a)(i)     10\n",
            "6  Legacy 2.1(b)(i)     11\n",
            "\n",
            "✓ Data normalized\n",
            "\n",
            "Normalized data:\n",
            "            SECTION  COUNT\n",
            "0        LEGACY 2.1     25\n",
            "1     LEGACY 2.1(A)     12\n",
            "2  LEGACY 2.1(A)(I)     10\n",
            "3     LEGACY 2.1(B)     13\n",
            "4  LEGACY 2.1(B)(I)     11\n",
            "5        LEGACY 2.2      8\n",
            "6        LEGACY 2.3      3\n",
            "\n",
            "✓ Data records built: 8 rows\n",
            "✓ Header record created\n",
            "\n",
            "✓ CSV file written: txseaIDEABDMRv000001.csv\n",
            "\n",
            "==================================================\n",
            "HEADER RECORD:\n",
            "         File_Type  Total_Records_in_File                File_Name              File_Identifier File_Reporting_Period Filler\n",
            "SEA PART B MED REQ                      8 txseaIDEABDMRv000001.csv FS228_TX_FFY2024_SY2425_V001             2024-2025       \n",
            "\n",
            "==================================================\n",
            "DATA RECORDS:\n",
            " File_Record_Number DG559_State_Code DG570_State_Agency_Number Filler1 Filler2         Table_Name Filler3   MR_Status       MR_Type  MR_Outcome Total_Indicator                                                                                      Explanation  MR_Count\n",
            "                  1               48                        01                 IDEABDISPRESMEDREQ            MEDTNHLD                                         N           Mediation requests resulting in mediations held as of the end of the reporting period.        25\n",
            "                  2               48                        01                 IDEABDISPRESMEDREQ                       MEDTNHLDDPH                           N                                     Mediations held that were related to due process complaints.        12\n",
            "                  3               48                        01                 IDEABDISPRESMEDREQ                       MEDTNHLDDPH MEDTNAGRMNT               N         Mediations held related to due process complaints that ended with a mediation agreement.        10\n",
            "                  4               48                        01                 IDEABDISPRESMEDREQ                     MEDTNHLDNODPH                           N                                 Mediations held that were not related to due process complaints.        13\n",
            "                  5               48                        01                 IDEABDISPRESMEDREQ                     MEDTNHLDNODPH MEDTNAGRMNT               N     Mediations held not related to due process complaints that ended with a mediation agreement.        11\n",
            "                  6               48                        01                 IDEABDISPRESMEDREQ            MEDTNPND                                         N Mediations pending as of the end of the reporting period (including mediation requests pending).         8\n",
            "                  7               48                        01                 IDEABDISPRESMEDREQ         MEDTNWTHDRW                                         N                  Mediation requests withdrawn or not held as of the end of the reporting period.         3\n",
            "                  8               48                        01                 IDEABDISPRESMEDREQ                                                             Y                                                                        Total mediation requests.        36\n",
            "\n",
            "==================================================\n",
            "QA SUMMARY:\n",
            "Total records: 8\n",
            "Detail records (Total_Indicator='N'): 7\n",
            "EUT record (Total_Indicator='Y'): 1\n",
            "\n",
            "Category Distribution:\n",
            "MR_Status populated: 3 rows\n",
            "MR_Type populated: 4 rows\n",
            "MR_Outcome populated: 2 rows\n",
            "\n",
            "MR_Count Distribution:\n",
            "MR_Count\n",
            "3     1\n",
            "8     1\n",
            "10    1\n",
            "11    1\n",
            "12    1\n",
            "13    1\n",
            "25    1\n",
            "36    1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "==================================================\n",
            "Downloading output file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_23a44cf5-194c-44dc-9edb-383937c2d0d9\", \"txseaIDEABDMRv000001.csv\", 1092)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FS229 – Due Process Complaints**"
      ],
      "metadata": {
        "id": "pOmvRCYQ26e_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PROGRAM:  fs229_due_process_complaints.py\n",
        "PURPOSE:  Import legacy SECTION/COUNT, create FS229 Header/Data tables via IF-THEN-DO,\n",
        "          then write the EDPass CSV (header record + data records; no column-name row).\n",
        "DATE:     2025-08-10\n",
        "AUTHOR:   Zane Wubbena, PhD\n",
        "CONTACT   zane.wubbena@tea.texas.gov\n",
        "INPUT:    CSV with columns: SECTION, COUNT\n",
        "OUTPUT:   UTF-8, CRLF CSV for EDPass (FS229 table: IDEABDISPRESDPC)\n",
        "\n",
        "SPEC BASIS: FS229 v21.0 (SY 2024-25)\n",
        "  - File Type: \"SEA PART B DUE PROCESS COMPLAINTS\"\n",
        "  - Table Name: IDEABDISPRESDPC\n",
        "  - Naming token (filename): IDEABDPC\n",
        "  - Category sets & permitted values:\n",
        "      * Due Process Complaint Status (Set A): HRNGADJCTD, DPCPND, DPCWTHDRW\n",
        "      * Hearings Fully Adjudicated Outcome (Set B): DCSNSWTHNTMLN, DCSNWNEXTTMLN\n",
        "      * Resolution Meeting Outcome (Set C): WRTNSTLAGRMNT\n",
        "      * Due Process Complaint Result (Set D): RSLTNMTG\n",
        "  - EUT = HRNGADJCTD + DPCPND + DPCWTHDRW, unless any is -1 → EUT = -1.\n",
        "  - Total_Indicator = 'N' for category sets A–D; 'Y' for EUT.\n",
        "\n",
        "RULES:\n",
        "  - Use 0 for true zeros; use -1 for missing/unknown.\n",
        "  - Exactly one category column populated per detail row; EUT has all category columns blank.\n",
        "  - Legacy → FS229 crosswalk:\n",
        "      \"Legacy 3.2\"     → DPC_Status=HRNGADJCTD\n",
        "      \"Legacy 3.2(a)\"  → HRNG_Outcome=DCSNSWTHNTMLN\n",
        "      \"Legacy 3.2(b)\"  → HRNG_Outcome=DCSNWNEXTTMLN\n",
        "      \"Legacy 3.3\"     → DPC_Status=DPCPND\n",
        "      \"Legacy 3.4\"     → DPC_Status=DPCWTHDRW\n",
        "      \"Legacy 3.1\"     → DPC_Result=RSLTNMTG\n",
        "      \"Legacy 3.1(a)\"  → RSLTN_Outcome=WRTNSTLAGRMNT\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# =========================\n",
        "# Step 0: Configuration\n",
        "# =========================\n",
        "\n",
        "# Configuration parameters\n",
        "CONFIG = {\n",
        "    'OUT_FILE': 'txseaIDEABDPCv000001.csv',  # must match header File Name (<=25 chars)\n",
        "    'FILE_ID': 'FS229_TX_FFY2024_SY2425_V001',  # File Identifier (<=32 chars)\n",
        "    'REPORT_PERIOD': '2024-2025',  # CCYY-CCYY\n",
        "    'STATE_CODE': 48,  # DG559 ANSI (2-digit, zero-padded)\n",
        "    'STATE_AGENCY': 1,  # DG570 (SEA=01)\n",
        "    'TABLE_NAME': 'IDEABDISPRESDPC',  # FS229 technical table name\n",
        "}\n",
        "\n",
        "# Explanations used in Data Records\n",
        "EXPLANATIONS = {\n",
        "    'STAT_ADJ': 'Due process complaints resulting in hearings fully adjudicated as of the end of the reporting period.',\n",
        "    'STAT_PND': 'Due process complaints pending as of the end of the reporting period.',\n",
        "    'STAT_WD': 'Due process complaints withdrawn or dismissed as of the end of the reporting period.',\n",
        "    'OUT_WTLN': 'Hearings fully adjudicated with decisions within the timeline.',\n",
        "    'OUT_XTLN': 'Hearings fully adjudicated with decisions within extended timeline.',\n",
        "    'RES_TOTAL': 'Due process complaints that resulted in resolution meetings by the end of the reporting period.',\n",
        "    'RES_WSA': 'Resolution meetings that resulted in written settlement agreements by the end of the reporting period.',\n",
        "    'TOTAL': 'Total due process complaints.',\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# Step 0a: Guards\n",
        "# =========================\n",
        "\n",
        "def validate_config():\n",
        "    \"\"\"Validate configuration parameters meet length requirements.\"\"\"\n",
        "    if len(CONFIG['FILE_ID']) > 32:\n",
        "        raise ValueError(f\"FILE_ID exceeds 32 characters: {CONFIG['FILE_ID']}, length={len(CONFIG['FILE_ID'])} (max=32)\")\n",
        "\n",
        "    if len(CONFIG['OUT_FILE']) > 25:\n",
        "        raise ValueError(f\"OUT_FILE exceeds 25 characters: {CONFIG['OUT_FILE']}, length={len(CONFIG['OUT_FILE'])} (max=25)\")\n",
        "\n",
        "# =========================\n",
        "# Step 1: Create Test Data\n",
        "# =========================\n",
        "\n",
        "def create_test_data():\n",
        "    \"\"\"Create comprehensive test dataset for FS229 with all sections present.\"\"\"\n",
        "    test_data = \"\"\"SECTION,COUNT\n",
        "Legacy 3.2,16\n",
        "Legacy 3.3,116\n",
        "Legacy 3.4,291\n",
        "Legacy 3.2(a),4\n",
        "Legacy 3.2(b),12\n",
        "Legacy 3.1,146\n",
        "Legacy 3.1(a),57\"\"\"\n",
        "\n",
        "    return pd.read_csv(StringIO(test_data))\n",
        "\n",
        "def create_test_data_with_missing():\n",
        "    \"\"\"Create test dataset with missing sections to test gap-filling logic.\"\"\"\n",
        "    test_data = \"\"\"SECTION,COUNT\n",
        "Legacy 3.2,16\n",
        "Legacy 3.4,292\n",
        "Legacy 3.2(b),12\n",
        "Legacy 3.1(a),57\"\"\"\n",
        "\n",
        "    return pd.read_csv(StringIO(test_data))\n",
        "\n",
        "def create_test_data_with_negatives():\n",
        "    \"\"\"Create test dataset with negative/missing values to test -1 handling.\"\"\"\n",
        "    test_data = \"\"\"SECTION,COUNT\n",
        "Legacy 3.2,-16\n",
        "Legacy 3.3,\n",
        "Legacy 3.4,292\n",
        "Legacy 3.2(a),4\n",
        "Legacy 3.2(b),12\n",
        "Legacy 3.1,-146\n",
        "Legacy 3.1(a),\"\"\"\n",
        "\n",
        "    return pd.read_csv(StringIO(test_data))\n",
        "\n",
        "def create_test_data_edge_cases():\n",
        "    \"\"\"Create test dataset with edge cases including duplicates and zeros.\"\"\"\n",
        "    test_data = \"\"\"SECTION,COUNT\n",
        "Legacy 3.2,16\n",
        "Legacy 3.2,16\n",
        "Legacy 3.3,0\n",
        "Legacy 3.4,292\n",
        "Legacy 3.2(a),4\n",
        "Legacy 3.2(b),12\n",
        "Legacy 3.1,146\n",
        "Legacy 3.1(a),57\"\"\"\n",
        "\n",
        "    return pd.read_csv(StringIO(test_data))\n",
        "\n",
        "# =========================\n",
        "# Step 2: Process Data\n",
        "# =========================\n",
        "\n",
        "def normalize_data(df):\n",
        "    \"\"\"\n",
        "    Normalize SECTION text and coerce COUNT to numeric.\n",
        "    Aggregate duplicates by SECTION (sum).\n",
        "    Enforce -1 for missing/negative values.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Normalize SECTION to uppercase and strip whitespace\n",
        "    df['SECTION'] = df['SECTION'].str.strip().str.upper()\n",
        "\n",
        "    # Coerce COUNT to numeric\n",
        "    df['COUNT'] = pd.to_numeric(df['COUNT'], errors='coerce')\n",
        "\n",
        "    # Aggregate by SECTION (sum duplicates)\n",
        "    df = df.groupby('SECTION', as_index=False)['COUNT'].sum()\n",
        "\n",
        "    # Replace missing or negative values with -1\n",
        "    df.loc[df['COUNT'].isna() | (df['COUNT'] < 0), 'COUNT'] = -1\n",
        "\n",
        "    # Round to integers\n",
        "    df['COUNT'] = df['COUNT'].round(0).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "def build_data_records(df):\n",
        "    \"\"\"Build data records (detail) with IF-THEN-DO logic for FS229.\"\"\"\n",
        "    records = []\n",
        "\n",
        "    # Track which required sections appeared\n",
        "    found_sections = {\n",
        "        'stat_32': False,   # Legacy 3.2 - HRNGADJCTD\n",
        "        'stat_33': False,   # Legacy 3.3 - DPCPND\n",
        "        'stat_34': False,   # Legacy 3.4 - DPCWTHDRW\n",
        "        'out_32a': False,   # Legacy 3.2(a) - DCSNSWTHNTMLN\n",
        "        'out_32b': False,   # Legacy 3.2(b) - DCSNWNEXTTMLN\n",
        "        'res_31': False,    # Legacy 3.1 - RSLTNMTG\n",
        "        'res_31a': False,   # Legacy 3.1(a) - WRTNSTLAGRMNT\n",
        "    }\n",
        "\n",
        "    # Track sum/missing for EUT (only Status rows contribute to total)\n",
        "    sum_status = 0\n",
        "    any_missing = False\n",
        "\n",
        "    # Process each row in the input data\n",
        "    for _, row in df.iterrows():\n",
        "        section = row['SECTION']\n",
        "        count = row['COUNT']\n",
        "\n",
        "        # Base record structure\n",
        "        record = {\n",
        "            'DG559_State_Code': f\"{CONFIG['STATE_CODE']:02d}\",\n",
        "            'DG570_State_Agency_Number': f\"{CONFIG['STATE_AGENCY']:02d}\",\n",
        "            'Filler1': '',\n",
        "            'Filler2': '',\n",
        "            'Table_Name': CONFIG['TABLE_NAME'],\n",
        "            'Filler3': '',\n",
        "            'DPC_Status': '',\n",
        "            'HRNG_Outcome': '',\n",
        "            'DPC_Result': '',\n",
        "            'RSLTN_Outcome': '',\n",
        "            'Total_Indicator': 'N',\n",
        "            'Explanation': '',\n",
        "            'DPC_Count': count\n",
        "        }\n",
        "\n",
        "        # Legacy → FS229 mapping (one output row per legacy line)\n",
        "        if section == 'LEGACY 3.2':  # Status: Hearings fully adjudicated\n",
        "            found_sections['stat_32'] = True\n",
        "            record['DPC_Status'] = 'HRNGADJCTD'\n",
        "            record['Explanation'] = EXPLANATIONS['STAT_ADJ']\n",
        "            if count == -1:\n",
        "                any_missing = True\n",
        "            else:\n",
        "                sum_status += count\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'LEGACY 3.3':  # Status: Pending\n",
        "            found_sections['stat_33'] = True\n",
        "            record['DPC_Status'] = 'DPCPND'\n",
        "            record['Explanation'] = EXPLANATIONS['STAT_PND']\n",
        "            if count == -1:\n",
        "                any_missing = True\n",
        "            else:\n",
        "                sum_status += count\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'LEGACY 3.4':  # Status: Withdrawn/Dismissed\n",
        "            found_sections['stat_34'] = True\n",
        "            record['DPC_Status'] = 'DPCWTHDRW'\n",
        "            record['Explanation'] = EXPLANATIONS['STAT_WD']\n",
        "            if count == -1:\n",
        "                any_missing = True\n",
        "            else:\n",
        "                sum_status += count\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'LEGACY 3.2(A)':  # Outcome: Decisions within timeline\n",
        "            found_sections['out_32a'] = True\n",
        "            record['HRNG_Outcome'] = 'DCSNSWTHNTMLN'\n",
        "            record['Explanation'] = EXPLANATIONS['OUT_WTLN']\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'LEGACY 3.2(B)':  # Outcome: Decisions within extended timeline\n",
        "            found_sections['out_32b'] = True\n",
        "            record['HRNG_Outcome'] = 'DCSNWNEXTTMLN'\n",
        "            record['Explanation'] = EXPLANATIONS['OUT_XTLN']\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'LEGACY 3.1':  # Result: Resolution meetings (total)\n",
        "            found_sections['res_31'] = True\n",
        "            record['DPC_Result'] = 'RSLTNMTG'\n",
        "            record['Explanation'] = EXPLANATIONS['RES_TOTAL']\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'LEGACY 3.1(A)':  # Resolution meeting outcome: Written settlement agreements\n",
        "            found_sections['res_31a'] = True\n",
        "            record['RSLTN_Outcome'] = 'WRTNSTLAGRMNT'\n",
        "            record['Explanation'] = EXPLANATIONS['RES_WSA']\n",
        "            records.append(record)\n",
        "\n",
        "        # Otherwise ignore unmapped sections (could log if desired)\n",
        "\n",
        "    # At EOF: add any missing required rows; then add EUT\n",
        "\n",
        "    # Category Set A: Status (3) - these affect the EUT calculation\n",
        "    if not found_sections['stat_32']:\n",
        "        any_missing = True\n",
        "        record = create_base_record()\n",
        "        record['DPC_Status'] = 'HRNGADJCTD'\n",
        "        record['Explanation'] = EXPLANATIONS['STAT_ADJ']\n",
        "        record['DPC_Count'] = -1\n",
        "        records.append(record)\n",
        "\n",
        "    if not found_sections['stat_33']:\n",
        "        any_missing = True\n",
        "        record = create_base_record()\n",
        "        record['DPC_Status'] = 'DPCPND'\n",
        "        record['Explanation'] = EXPLANATIONS['STAT_PND']\n",
        "        record['DPC_Count'] = -1\n",
        "        records.append(record)\n",
        "\n",
        "    if not found_sections['stat_34']:\n",
        "        any_missing = True\n",
        "        record = create_base_record()\n",
        "        record['DPC_Status'] = 'DPCWTHDRW'\n",
        "        record['Explanation'] = EXPLANATIONS['STAT_WD']\n",
        "        record['DPC_Count'] = -1\n",
        "        records.append(record)\n",
        "\n",
        "    # Category Set B: Fully adjudicated outcome (2)\n",
        "    if not found_sections['out_32a']:\n",
        "        record = create_base_record()\n",
        "        record['HRNG_Outcome'] = 'DCSNSWTHNTMLN'\n",
        "        record['Explanation'] = EXPLANATIONS['OUT_WTLN']\n",
        "        record['DPC_Count'] = -1\n",
        "        records.append(record)\n",
        "\n",
        "    if not found_sections['out_32b']:\n",
        "        record = create_base_record()\n",
        "        record['HRNG_Outcome'] = 'DCSNWNEXTTMLN'\n",
        "        record['Explanation'] = EXPLANATIONS['OUT_XTLN']\n",
        "        record['DPC_Count'] = -1\n",
        "        records.append(record)\n",
        "\n",
        "    # Category Set C: Resolution meeting outcome (1)\n",
        "    if not found_sections['res_31a']:\n",
        "        record = create_base_record()\n",
        "        record['RSLTN_Outcome'] = 'WRTNSTLAGRMNT'\n",
        "        record['Explanation'] = EXPLANATIONS['RES_WSA']\n",
        "        record['DPC_Count'] = -1\n",
        "        records.append(record)\n",
        "\n",
        "    # Category Set D: Due process complaint result (1)\n",
        "    if not found_sections['res_31']:\n",
        "        record = create_base_record()\n",
        "        record['DPC_Result'] = 'RSLTNMTG'\n",
        "        record['Explanation'] = EXPLANATIONS['RES_TOTAL']\n",
        "        record['DPC_Count'] = -1\n",
        "        records.append(record)\n",
        "\n",
        "    # Education Unit Total (EUT) row\n",
        "    eut_record = create_base_record()\n",
        "    eut_record['Total_Indicator'] = 'Y'\n",
        "    eut_record['Explanation'] = EXPLANATIONS['TOTAL']\n",
        "    eut_record['DPC_Count'] = -1 if any_missing else sum_status\n",
        "    records.append(eut_record)\n",
        "\n",
        "    # Create DataFrame and add File_Record_Number\n",
        "    df_records = pd.DataFrame(records)\n",
        "    df_records.insert(0, 'File_Record_Number', range(1, len(df_records) + 1))\n",
        "\n",
        "    return df_records\n",
        "\n",
        "def create_base_record():\n",
        "    \"\"\"Create a base record with all fields initialized.\"\"\"\n",
        "    return {\n",
        "        'DG559_State_Code': f\"{CONFIG['STATE_CODE']:02d}\",\n",
        "        'DG570_State_Agency_Number': f\"{CONFIG['STATE_AGENCY']:02d}\",\n",
        "        'Filler1': '',\n",
        "        'Filler2': '',\n",
        "        'Table_Name': CONFIG['TABLE_NAME'],\n",
        "        'Filler3': '',\n",
        "        'DPC_Status': '',\n",
        "        'HRNG_Outcome': '',\n",
        "        'DPC_Result': '',\n",
        "        'RSLTN_Outcome': '',\n",
        "        'Total_Indicator': 'N',\n",
        "        'Explanation': '',\n",
        "        'DPC_Count': 0\n",
        "    }\n",
        "\n",
        "def create_header_record(n_records):\n",
        "    \"\"\"Create the header record (1 row) for FS229.\"\"\"\n",
        "    header = {\n",
        "        'File_Type': 'SEA PART B DUE PROCESS COMPLAINTS',  # FS229 specific header\n",
        "        'Total_Records_in_File': n_records,\n",
        "        'File_Name': CONFIG['OUT_FILE'],\n",
        "        'File_Identifier': CONFIG['FILE_ID'],\n",
        "        'File_Reporting_Period': CONFIG['REPORT_PERIOD'],\n",
        "        'Filler': ''\n",
        "    }\n",
        "    return pd.DataFrame([header])\n",
        "\n",
        "def write_csv(header_df, data_df, filename):\n",
        "    \"\"\"Write final CSV (values only; no column headers).\"\"\"\n",
        "    # Write header row\n",
        "    header_csv = header_df.to_csv(index=False, header=False, lineterminator='\\r\\n')\n",
        "\n",
        "    # Write data rows\n",
        "    data_csv = data_df.to_csv(index=False, header=False, lineterminator='\\r\\n')\n",
        "\n",
        "    # Combine\n",
        "    final_csv = header_csv + data_csv\n",
        "\n",
        "    # Save to file\n",
        "    with open(filename, 'w', encoding='utf-8', newline='') as f:\n",
        "        f.write(final_csv)\n",
        "\n",
        "    return final_csv\n",
        "\n",
        "# =========================\n",
        "# QA Functions\n",
        "# =========================\n",
        "\n",
        "def perform_qa_checks(df_data):\n",
        "    \"\"\"Perform quality assurance checks on the data records.\"\"\"\n",
        "    qa_results = {}\n",
        "\n",
        "    # Check total record count\n",
        "    qa_results['total_records'] = len(df_data)\n",
        "    qa_results['detail_records'] = len(df_data[df_data['Total_Indicator'] == 'N'])\n",
        "    qa_results['eut_records'] = len(df_data[df_data['Total_Indicator'] == 'Y'])\n",
        "\n",
        "    # Check category distribution\n",
        "    qa_results['dpc_status_count'] = len(df_data[df_data['DPC_Status'] != ''])\n",
        "    qa_results['hrng_outcome_count'] = len(df_data[df_data['HRNG_Outcome'] != ''])\n",
        "    qa_results['dpc_result_count'] = len(df_data[df_data['DPC_Result'] != ''])\n",
        "    qa_results['rsltn_outcome_count'] = len(df_data[df_data['RSLTN_Outcome'] != ''])\n",
        "\n",
        "    # Check for missing values (-1)\n",
        "    qa_results['missing_count'] = len(df_data[df_data['DPC_Count'] == -1])\n",
        "\n",
        "    # Verify exactly one category per detail row (except EUT)\n",
        "    detail_rows = df_data[df_data['Total_Indicator'] == 'N']\n",
        "    categories_per_row = []\n",
        "    for _, row in detail_rows.iterrows():\n",
        "        cat_count = sum([\n",
        "            1 if row['DPC_Status'] != '' else 0,\n",
        "            1 if row['HRNG_Outcome'] != '' else 0,\n",
        "            1 if row['DPC_Result'] != '' else 0,\n",
        "            1 if row['RSLTN_Outcome'] != '' else 0\n",
        "        ])\n",
        "        categories_per_row.append(cat_count)\n",
        "\n",
        "    qa_results['single_category_rows'] = categories_per_row.count(1)\n",
        "    qa_results['multi_category_rows'] = len([c for c in categories_per_row if c > 1])\n",
        "    qa_results['no_category_rows'] = categories_per_row.count(0)\n",
        "\n",
        "    return qa_results\n",
        "\n",
        "# =========================\n",
        "# Main Execution\n",
        "# =========================\n",
        "\n",
        "def main(test_scenario='complete'):\n",
        "    \"\"\"\n",
        "    Main execution function.\n",
        "\n",
        "    Args:\n",
        "        test_scenario: Which test dataset to use\n",
        "            - 'complete': All sections present with valid data\n",
        "            - 'missing': Some sections missing to test gap-filling\n",
        "            - 'negatives': Test handling of negative/missing values\n",
        "            - 'edge_cases': Test duplicates and zero values\n",
        "            - 'upload': Upload your own file\n",
        "    \"\"\"\n",
        "    print(\"FS229 Due Process Complaints Build Script\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Validate configuration\n",
        "    validate_config()\n",
        "    print(\"✓ Configuration validated\")\n",
        "\n",
        "    # Select test data based on scenario\n",
        "    if test_scenario == 'complete':\n",
        "        print(\"\\nUsing COMPLETE test data (all sections present)...\")\n",
        "        df_input = create_test_data()\n",
        "    elif test_scenario == 'missing':\n",
        "        print(\"\\nUsing test data with MISSING sections...\")\n",
        "        df_input = create_test_data_with_missing()\n",
        "    elif test_scenario == 'negatives':\n",
        "        print(\"\\nUsing test data with NEGATIVE/MISSING values...\")\n",
        "        df_input = create_test_data_with_negatives()\n",
        "    elif test_scenario == 'edge_cases':\n",
        "        print(\"\\nUsing test data with EDGE CASES (duplicates, zeros)...\")\n",
        "        df_input = create_test_data_edge_cases()\n",
        "    elif test_scenario == 'upload':\n",
        "        print(\"\\nPlease upload your input CSV file...\")\n",
        "        uploaded = files.upload()\n",
        "        filename = list(uploaded.keys())[0]\n",
        "        df_input = pd.read_csv(filename)\n",
        "    else:\n",
        "        print(f\"\\nUnknown scenario '{test_scenario}'. Using complete test data...\")\n",
        "        df_input = create_test_data()\n",
        "\n",
        "    print(f\"\\nInput data shape: {df_input.shape}\")\n",
        "    print(\"\\nInput data preview:\")\n",
        "    print(df_input)\n",
        "\n",
        "    # Step 1: Normalize data\n",
        "    df_normalized = normalize_data(df_input)\n",
        "    print(\"\\n✓ Data normalized\")\n",
        "    print(\"\\nNormalized data:\")\n",
        "    print(df_normalized)\n",
        "\n",
        "    # Step 2: Build data records\n",
        "    df_data = build_data_records(df_normalized)\n",
        "    print(f\"\\n✓ Data records built: {len(df_data)} rows\")\n",
        "\n",
        "    # Step 3: Create header record\n",
        "    df_header = create_header_record(len(df_data))\n",
        "    print(\"✓ Header record created\")\n",
        "\n",
        "    # Step 4: Write CSV\n",
        "    output_filename = CONFIG['OUT_FILE']\n",
        "    write_csv(df_header, df_data, output_filename)\n",
        "    print(f\"\\n✓ CSV file written: {output_filename}\")\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"HEADER RECORD:\")\n",
        "    print(df_header.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"DATA RECORDS:\")\n",
        "    print(df_data.to_string(index=False))\n",
        "\n",
        "    # QA Summary\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"QA SUMMARY:\")\n",
        "    qa_results = perform_qa_checks(df_data)\n",
        "    for key, value in qa_results.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    # Check expected structure\n",
        "    print(\"\\nExpected Structure Validation:\")\n",
        "    expected_detail = 7\n",
        "    expected_total = 8\n",
        "\n",
        "    if qa_results['detail_records'] == expected_detail:\n",
        "        print(f\"  ✓ Detail records: {qa_results['detail_records']} (expected {expected_detail})\")\n",
        "    else:\n",
        "        print(f\"  ✗ Detail records: {qa_results['detail_records']} (expected {expected_detail})\")\n",
        "\n",
        "    if qa_results['total_records'] == expected_total:\n",
        "        print(f\"  ✓ Total records: {qa_results['total_records']} (expected {expected_total})\")\n",
        "    else:\n",
        "        print(f\"  ✗ Total records: {qa_results['total_records']} (expected {expected_total})\")\n",
        "\n",
        "    if qa_results['single_category_rows'] == expected_detail:\n",
        "        print(f\"  ✓ Single category per detail row: {qa_results['single_category_rows']}/{expected_detail}\")\n",
        "    else:\n",
        "        print(f\"  ✗ Single category per detail row: {qa_results['single_category_rows']}/{expected_detail}\")\n",
        "\n",
        "    # Value distribution\n",
        "    print(\"\\nDPC_Count Distribution:\")\n",
        "    print(df_data['DPC_Count'].value_counts().sort_index())\n",
        "\n",
        "    # Download the file\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Downloading output file...\")\n",
        "    files.download(output_filename)\n",
        "\n",
        "    return df_header, df_data\n",
        "\n",
        "# Run the main function with different test scenarios\n",
        "if __name__ == \"__main__\":\n",
        "    # Choose which test scenario to run:\n",
        "    # 'complete' - all sections with valid data\n",
        "    # 'missing' - some sections missing\n",
        "    # 'negatives' - test -1 handling\n",
        "    # 'edge_cases' - duplicates and zeros\n",
        "    # 'upload' - upload your own file\n",
        "\n",
        "    header_df, data_df = main(test_scenario='complete')\n",
        "\n",
        "    # Uncomment to test other scenarios:\n",
        "    # header_df, data_df = main(test_scenario='missing')\n",
        "    # header_df, data_df = main(test_scenario='negatives')\n",
        "    # header_df, data_df = main(test_scenario='edge_cases')\n",
        "    # header_df, data_df = main(test_scenario='upload')\n",
        "\n",
        "# =========================\n",
        "#  Python Program End\n",
        "# ========================="
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gUxuammH2-dS",
        "outputId": "6fcc1c5d-92b6-4a77-a6c7-25cbaffd3014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FS229 Due Process Complaints Build Script\n",
            "==================================================\n",
            "✓ Configuration validated\n",
            "\n",
            "Using COMPLETE test data (all sections present)...\n",
            "\n",
            "Input data shape: (7, 2)\n",
            "\n",
            "Input data preview:\n",
            "         SECTION  COUNT\n",
            "0     Legacy 3.2     16\n",
            "1     Legacy 3.3    116\n",
            "2     Legacy 3.4    291\n",
            "3  Legacy 3.2(a)      4\n",
            "4  Legacy 3.2(b)     12\n",
            "5     Legacy 3.1    146\n",
            "6  Legacy 3.1(a)     57\n",
            "\n",
            "✓ Data normalized\n",
            "\n",
            "Normalized data:\n",
            "         SECTION  COUNT\n",
            "0     LEGACY 3.1    146\n",
            "1  LEGACY 3.1(A)     57\n",
            "2     LEGACY 3.2     16\n",
            "3  LEGACY 3.2(A)      4\n",
            "4  LEGACY 3.2(B)     12\n",
            "5     LEGACY 3.3    116\n",
            "6     LEGACY 3.4    291\n",
            "\n",
            "✓ Data records built: 8 rows\n",
            "✓ Header record created\n",
            "\n",
            "✓ CSV file written: txseaIDEABDPCv000001.csv\n",
            "\n",
            "==================================================\n",
            "HEADER RECORD:\n",
            "                        File_Type  Total_Records_in_File                File_Name              File_Identifier File_Reporting_Period Filler\n",
            "SEA PART B DUE PROCESS COMPLAINTS                      8 txseaIDEABDPCv000001.csv FS229_TX_FFY2024_SY2425_V001             2024-2025       \n",
            "\n",
            "==================================================\n",
            "DATA RECORDS:\n",
            " File_Record_Number DG559_State_Code DG570_State_Agency_Number Filler1 Filler2      Table_Name Filler3 DPC_Status  HRNG_Outcome DPC_Result RSLTN_Outcome Total_Indicator                                                                                            Explanation  DPC_Count\n",
            "                  1               48                        01                 IDEABDISPRESDPC                                    RSLTNMTG                             N        Due process complaints that resulted in resolution meetings by the end of the reporting period.        146\n",
            "                  2               48                        01                 IDEABDISPRESDPC                                             WRTNSTLAGRMNT               N Resolution meetings that resulted in written settlement agreements by the end of the reporting period.         57\n",
            "                  3               48                        01                 IDEABDISPRESDPC         HRNGADJCTD                                                      N  Due process complaints resulting in hearings fully adjudicated as of the end of the reporting period.         16\n",
            "                  4               48                        01                 IDEABDISPRESDPC                    DCSNSWTHNTMLN                                        N                                         Hearings fully adjudicated with decisions within the timeline.          4\n",
            "                  5               48                        01                 IDEABDISPRESDPC                    DCSNWNEXTTMLN                                        N                                    Hearings fully adjudicated with decisions within extended timeline.         12\n",
            "                  6               48                        01                 IDEABDISPRESDPC             DPCPND                                                      N                                  Due process complaints pending as of the end of the reporting period.        116\n",
            "                  7               48                        01                 IDEABDISPRESDPC          DPCWTHDRW                                                      N                   Due process complaints withdrawn or dismissed as of the end of the reporting period.        291\n",
            "                  8               48                        01                 IDEABDISPRESDPC                                                                         Y                                                                          Total due process complaints.        423\n",
            "\n",
            "==================================================\n",
            "QA SUMMARY:\n",
            "  total_records: 8\n",
            "  detail_records: 7\n",
            "  eut_records: 1\n",
            "  dpc_status_count: 3\n",
            "  hrng_outcome_count: 2\n",
            "  dpc_result_count: 1\n",
            "  rsltn_outcome_count: 1\n",
            "  missing_count: 0\n",
            "  single_category_rows: 7\n",
            "  multi_category_rows: 0\n",
            "  no_category_rows: 0\n",
            "\n",
            "Expected Structure Validation:\n",
            "  ✓ Detail records: 7 (expected 7)\n",
            "  ✓ Total records: 8 (expected 8)\n",
            "  ✓ Single category per detail row: 7/7\n",
            "\n",
            "DPC_Count Distribution:\n",
            "DPC_Count\n",
            "4      1\n",
            "12     1\n",
            "16     1\n",
            "57     1\n",
            "116    1\n",
            "146    1\n",
            "291    1\n",
            "423    1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "==================================================\n",
            "Downloading output file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6277d51f-036c-41c9-a626-bc50af75f975\", \"txseaIDEABDPCv000001.csv\", 1090)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FS230 – Expedited Due Process Complaints**"
      ],
      "metadata": {
        "id": "pBIx8H802_fF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PROGRAM:  fs230_expedited_due_process_complaints.py\n",
        "PURPOSE:  Import legacy SECTION/COUNT, create FS230 Header/Data tables via IF-THEN-DO,\n",
        "          then write the EDPass CSV (header record + data records; no column-name row).\n",
        "DATE:     2025-08-10\n",
        "AUTHOR:   Zane Wubbena, PhD\n",
        "CONTACT   zane.wubbena@tea.texas.gov\n",
        "INPUT:    CSV with columns: SECTION, COUNT\n",
        "OUTPUT:   UTF-8, CRLF CSV for EDPass (FS230 table: IDEABDISPRESEDPC)\n",
        "\n",
        "SPEC BASIS: FS230 v21.0 (SY 2024-25)\n",
        "  - File Type: \"SEA PART B EXPEDITED DUE PROCESS COMPLAINTS\"\n",
        "  - Table Name: IDEABDISPRESEDPC\n",
        "  - File naming token: IDEABEDPC  (e.g., txseaIDEABEDPCv000001.csv)  [max 25 chars inc. \".csv\"]\n",
        "  - Category sets & permitted values:\n",
        "      * Category Set A (Status): EXPHRNGADJCTD, EXPDPCPND, EXPDPCWTHDRW\n",
        "      * Category Set B (Expedited hearing fully adjudicated outcome): CHNGPLCMTORDRD\n",
        "      * Category Set C (Expedited resolution meeting outcome): EXPWRTNSTMTAGMT\n",
        "      * Category Set D (Expedited due process complaint result): EXPRSLTNMTG\n",
        "  - Total_Indicator: 'N' for category sets A–D; 'Y' for Education Unit Total (EUT).\n",
        "  - EUT = EXPHRNGADJCTD + EXPDPCPND + EXPDPCWTHDRW; if any of the three is -1 => EUT = -1.\n",
        "\n",
        "RULES:\n",
        "  - Zero counts required; use -1 to report missing counts.\n",
        "  - Exactly one category column populated per detail row; EUT has category columns blank.\n",
        "  - Legacy → FS230 crosswalk used:\n",
        "      \"Legacy 4.2\"     → Status=EXPHRNGADJCTD\n",
        "      \"Legacy 4.3\"     → Status=EXPDPCPND\n",
        "      \"Legacy 4.4\"     → Status=EXPDPCWTHDRW\n",
        "      \"Legacy 4.2(a)\"  → Expedited Hearing Outcome=CHNGPLCMTORDRD\n",
        "      \"Legacy 4.1\"     → Result=EXPRSLTNMTG\n",
        "      \"Legacy 4.1(a)\"  → Resolution Meeting Outcome=EXPWRTNSTMTAGMT\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# =========================\n",
        "# Step 0: Configuration\n",
        "# =========================\n",
        "\n",
        "# Configuration parameters\n",
        "CONFIG = {\n",
        "    'OUT_FILE': 'txseaIDEABEDPCv000001.csv',  # must match header File Name (<=25 chars)\n",
        "    'FILE_ID': 'FS230_TX_FFY2024_SY2425_V001',  # File Identifier (<=32 chars)\n",
        "    'REPORT_PERIOD': '2024-2025',  # CCYY-CCYY\n",
        "    'STATE_CODE': 48,  # DG559 (2-digit, zero-padded)\n",
        "    'STATE_AGENCY': 1,  # DG570 (SEA=01)\n",
        "    'TABLE_NAME': 'IDEABDISPRESEDPC',  # FS230 technical table name\n",
        "}\n",
        "\n",
        "# Explanations used in Data Records\n",
        "EXPLANATIONS = {\n",
        "    'STATUS_ADJ': 'Expedited due process complaints resulting in expedited hearings fully adjudicated as of the end of the reporting period.',\n",
        "    'STATUS_PND': 'Expedited due process complaints pending as of the end of the reporting period.',\n",
        "    'STATUS_WD': 'Expedited due process complaints withdrawn or dismissed as of the end of the reporting period.',\n",
        "    'OUT_CHGPLCMT': 'Expedited hearings fully adjudicated with change of placement ordered.',\n",
        "    'RES_TOTAL': 'Expedited due process complaints that resulted in expedited resolution meetings by the end of the reporting period.',\n",
        "    'RES_WSA': 'Expedited resolution meetings that resulted in written settlement agreements by the end of the reporting period.',\n",
        "    'TOTAL': 'Total expedited due process complaints.',\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# Step 0a: Guards\n",
        "# =========================\n",
        "\n",
        "def validate_config():\n",
        "    \"\"\"Validate configuration parameters meet length requirements.\"\"\"\n",
        "    if len(CONFIG['FILE_ID']) > 32:\n",
        "        raise ValueError(f\"FILE_ID exceeds 32 characters: {CONFIG['FILE_ID']}, length={len(CONFIG['FILE_ID'])} (max=32)\")\n",
        "\n",
        "    if len(CONFIG['OUT_FILE']) > 25:\n",
        "        raise ValueError(f\"OUT_FILE exceeds 25 characters: {CONFIG['OUT_FILE']}, length={len(CONFIG['OUT_FILE'])} (max=25)\")\n",
        "\n",
        "# =========================\n",
        "# Step 1: Create Test Data\n",
        "# =========================\n",
        "\n",
        "def create_test_data():\n",
        "    \"\"\"Create comprehensive test dataset for FS230 with all sections present.\"\"\"\n",
        "    test_data = \"\"\"SECTION,COUNT\n",
        "Legacy 4.2,4\n",
        "Legacy 4.3,5\n",
        "Legacy 4.4,70\n",
        "Legacy 4.2(a),0\n",
        "Legacy 4.1,43\n",
        "Legacy 4.1(a),21\"\"\"\n",
        "\n",
        "    return pd.read_csv(StringIO(test_data))\n",
        "\n",
        "def create_test_data_with_missing():\n",
        "    \"\"\"Create test dataset with missing sections to test gap-filling logic.\"\"\"\n",
        "    test_data = \"\"\"SECTION,COUNT\n",
        "Legacy 4.2,4\n",
        "Legacy 4.4,70\n",
        "Legacy 4.1(a),21\"\"\"\n",
        "\n",
        "    return pd.read_csv(StringIO(test_data))\n",
        "\n",
        "def create_test_data_with_negatives():\n",
        "    \"\"\"Create test dataset with negative/missing values to test -1 handling.\"\"\"\n",
        "    test_data = \"\"\"SECTION,COUNT\n",
        "Legacy 4.2,-4\n",
        "Legacy 4.3,\n",
        "Legacy 4.4,70\n",
        "Legacy 4.2(a),0\n",
        "Legacy 4.1,43\n",
        "Legacy 4.1(a),\"\"\"\n",
        "\n",
        "    return pd.read_csv(StringIO(test_data))\n",
        "\n",
        "def create_test_data_edge_cases():\n",
        "    \"\"\"Create test dataset with edge cases including duplicates and zeros.\"\"\"\n",
        "    test_data = \"\"\"SECTION,COUNT\n",
        "Legacy 4.2,5\n",
        "Legacy 4.2,3\n",
        "Legacy 4.3,0\n",
        "Legacy 4.4,4\n",
        "Legacy 4.2(a),2\n",
        "Legacy 4.1,0\n",
        "Legacy 4.1,6\n",
        "Legacy 4.1(a),0\"\"\"\n",
        "\n",
        "    return pd.read_csv(StringIO(test_data))\n",
        "\n",
        "def create_test_data_realistic():\n",
        "    \"\"\"Create realistic test dataset simulating actual expedited complaint data.\"\"\"\n",
        "    test_data = \"\"\"SECTION,COUNT\n",
        "Legacy 4.2,12\n",
        "Legacy 4.3,7\n",
        "Legacy 4.4,4\n",
        "Legacy 4.2(a),9\n",
        "Legacy 4.1,18\n",
        "Legacy 4.1(a),14\"\"\"\n",
        "\n",
        "    return pd.read_csv(StringIO(test_data))\n",
        "\n",
        "# =========================\n",
        "# Step 2: Process Data\n",
        "# =========================\n",
        "\n",
        "def normalize_data(df):\n",
        "    \"\"\"\n",
        "    Normalize SECTION text and coerce COUNT to numeric.\n",
        "    Aggregate duplicates by SECTION (sum).\n",
        "    Enforce -1 for missing/negative values.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Normalize SECTION to uppercase and strip whitespace\n",
        "    df['SECTION'] = df['SECTION'].str.strip().str.upper()\n",
        "\n",
        "    # Coerce COUNT to numeric\n",
        "    df['COUNT'] = pd.to_numeric(df['COUNT'], errors='coerce')\n",
        "\n",
        "    # Aggregate by SECTION (sum duplicates)\n",
        "    df = df.groupby('SECTION', as_index=False)['COUNT'].sum()\n",
        "\n",
        "    # Replace missing or negative values with -1\n",
        "    df.loc[df['COUNT'].isna() | (df['COUNT'] < 0), 'COUNT'] = -1\n",
        "\n",
        "    # Round to integers\n",
        "    df['COUNT'] = df['COUNT'].round(0).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "def build_data_records(df):\n",
        "    \"\"\"Build data records (detail) with IF-THEN-DO logic for FS230.\"\"\"\n",
        "    records = []\n",
        "\n",
        "    # Track which required sections appeared\n",
        "    found_sections = {\n",
        "        'stat_42': False,   # Legacy 4.2 - EXPHRNGADJCTD\n",
        "        'stat_43': False,   # Legacy 4.3 - EXPDPCPND\n",
        "        'stat_44': False,   # Legacy 4.4 - EXPDPCWTHDRW\n",
        "        'out_42a': False,   # Legacy 4.2(a) - CHNGPLCMTORDRD\n",
        "        'res_41': False,    # Legacy 4.1 - EXPRSLTNMTG\n",
        "        'res_41a': False,   # Legacy 4.1(a) - EXPWRTNSTMTAGMT\n",
        "    }\n",
        "\n",
        "    # Track sum/missing for EUT (only Status rows contribute to total)\n",
        "    sum_status = 0\n",
        "    any_missing = False\n",
        "\n",
        "    # Process each row in the input data\n",
        "    for _, row in df.iterrows():\n",
        "        section = row['SECTION']\n",
        "        count = row['COUNT']\n",
        "\n",
        "        # Base record structure\n",
        "        record = {\n",
        "            'DG559_State_Code': f\"{CONFIG['STATE_CODE']:02d}\",\n",
        "            'DG570_State_Agency_Number': f\"{CONFIG['STATE_AGENCY']:02d}\",\n",
        "            'Filler1': '',\n",
        "            'Filler2': '',\n",
        "            'Table_Name': CONFIG['TABLE_NAME'],\n",
        "            'Filler3': '',\n",
        "            'EXP_Status': '',\n",
        "            'EXP_Hrng_Outcome': '',\n",
        "            'EXP_Result': '',\n",
        "            'EXP_Rsltn_Outcome': '',\n",
        "            'Total_Indicator': 'N',\n",
        "            'Explanation': '',\n",
        "            'EXP_Count': count\n",
        "        }\n",
        "\n",
        "        # Legacy → FS230 mapping\n",
        "        if section == 'LEGACY 4.2':  # Status: Expedited hearings fully adjudicated\n",
        "            found_sections['stat_42'] = True\n",
        "            record['EXP_Status'] = 'EXPHRNGADJCTD'\n",
        "            record['Explanation'] = EXPLANATIONS['STATUS_ADJ']\n",
        "            if count == -1:\n",
        "                any_missing = True\n",
        "            else:\n",
        "                sum_status += count\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'LEGACY 4.3':  # Status: Pending\n",
        "            found_sections['stat_43'] = True\n",
        "            record['EXP_Status'] = 'EXPDPCPND'\n",
        "            record['Explanation'] = EXPLANATIONS['STATUS_PND']\n",
        "            if count == -1:\n",
        "                any_missing = True\n",
        "            else:\n",
        "                sum_status += count\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'LEGACY 4.4':  # Status: Withdrawn/Dismissed\n",
        "            found_sections['stat_44'] = True\n",
        "            record['EXP_Status'] = 'EXPDPCWTHDRW'\n",
        "            record['Explanation'] = EXPLANATIONS['STATUS_WD']\n",
        "            if count == -1:\n",
        "                any_missing = True\n",
        "            else:\n",
        "                sum_status += count\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'LEGACY 4.2(A)':  # Expedited hearing outcome: Change of placement ordered\n",
        "            found_sections['out_42a'] = True\n",
        "            record['EXP_Hrng_Outcome'] = 'CHNGPLCMTORDRD'\n",
        "            record['Explanation'] = EXPLANATIONS['OUT_CHGPLCMT']\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'LEGACY 4.1':  # Result: Expedited resolution meetings (total)\n",
        "            found_sections['res_41'] = True\n",
        "            record['EXP_Result'] = 'EXPRSLTNMTG'\n",
        "            record['Explanation'] = EXPLANATIONS['RES_TOTAL']\n",
        "            records.append(record)\n",
        "\n",
        "        elif section == 'LEGACY 4.1(A)':  # Resolution meeting outcome: Written settlement agreements\n",
        "            found_sections['res_41a'] = True\n",
        "            record['EXP_Rsltn_Outcome'] = 'EXPWRTNSTMTAGMT'\n",
        "            record['Explanation'] = EXPLANATIONS['RES_WSA']\n",
        "            records.append(record)\n",
        "\n",
        "        # Otherwise ignore unmapped sections (could log if desired)\n",
        "\n",
        "    # At EOF: add any missing required rows; then add EUT\n",
        "\n",
        "    # Category Set A: Status (3) - these affect the EUT calculation\n",
        "    if not found_sections['stat_42']:\n",
        "        any_missing = True\n",
        "        record = create_base_record()\n",
        "        record['EXP_Status'] = 'EXPHRNGADJCTD'\n",
        "        record['Explanation'] = EXPLANATIONS['STATUS_ADJ']\n",
        "        record['EXP_Count'] = -1\n",
        "        records.append(record)\n",
        "\n",
        "    if not found_sections['stat_43']:\n",
        "        any_missing = True\n",
        "        record = create_base_record()\n",
        "        record['EXP_Status'] = 'EXPDPCPND'\n",
        "        record['Explanation'] = EXPLANATIONS['STATUS_PND']\n",
        "        record['EXP_Count'] = -1\n",
        "        records.append(record)\n",
        "\n",
        "    if not found_sections['stat_44']:\n",
        "        any_missing = True\n",
        "        record = create_base_record()\n",
        "        record['EXP_Status'] = 'EXPDPCWTHDRW'\n",
        "        record['Explanation'] = EXPLANATIONS['STATUS_WD']\n",
        "        record['EXP_Count'] = -1\n",
        "        records.append(record)\n",
        "\n",
        "    # Category Set B: Expedited hearing fully adjudicated outcome (1)\n",
        "    if not found_sections['out_42a']:\n",
        "        record = create_base_record()\n",
        "        record['EXP_Hrng_Outcome'] = 'CHNGPLCMTORDRD'\n",
        "        record['Explanation'] = EXPLANATIONS['OUT_CHGPLCMT']\n",
        "        record['EXP_Count'] = -1\n",
        "        records.append(record)\n",
        "\n",
        "    # Category Set C: Expedited resolution meeting outcome (1)\n",
        "    if not found_sections['res_41a']:\n",
        "        record = create_base_record()\n",
        "        record['EXP_Rsltn_Outcome'] = 'EXPWRTNSTMTAGMT'\n",
        "        record['Explanation'] = EXPLANATIONS['RES_WSA']\n",
        "        record['EXP_Count'] = -1\n",
        "        records.append(record)\n",
        "\n",
        "    # Category Set D: Expedited due process complaint result (1)\n",
        "    if not found_sections['res_41']:\n",
        "        record = create_base_record()\n",
        "        record['EXP_Result'] = 'EXPRSLTNMTG'\n",
        "        record['Explanation'] = EXPLANATIONS['RES_TOTAL']\n",
        "        record['EXP_Count'] = -1\n",
        "        records.append(record)\n",
        "\n",
        "    # Education Unit Total (EUT) row\n",
        "    eut_record = create_base_record()\n",
        "    eut_record['Total_Indicator'] = 'Y'\n",
        "    eut_record['Explanation'] = EXPLANATIONS['TOTAL']\n",
        "    eut_record['EXP_Count'] = -1 if any_missing else sum_status\n",
        "    records.append(eut_record)\n",
        "\n",
        "    # Create DataFrame and add File_Record_Number\n",
        "    df_records = pd.DataFrame(records)\n",
        "    df_records.insert(0, 'File_Record_Number', range(1, len(df_records) + 1))\n",
        "\n",
        "    return df_records\n",
        "\n",
        "def create_base_record():\n",
        "    \"\"\"Create a base record with all fields initialized.\"\"\"\n",
        "    return {\n",
        "        'DG559_State_Code': f\"{CONFIG['STATE_CODE']:02d}\",\n",
        "        'DG570_State_Agency_Number': f\"{CONFIG['STATE_AGENCY']:02d}\",\n",
        "        'Filler1': '',\n",
        "        'Filler2': '',\n",
        "        'Table_Name': CONFIG['TABLE_NAME'],\n",
        "        'Filler3': '',\n",
        "        'EXP_Status': '',\n",
        "        'EXP_Hrng_Outcome': '',\n",
        "        'EXP_Result': '',\n",
        "        'EXP_Rsltn_Outcome': '',\n",
        "        'Total_Indicator': 'N',\n",
        "        'Explanation': '',\n",
        "        'EXP_Count': 0\n",
        "    }\n",
        "\n",
        "def create_header_record(n_records):\n",
        "    \"\"\"Create the header record (1 row) for FS230.\"\"\"\n",
        "    header = {\n",
        "        'File_Type': 'SEA PART B EXPEDITED DUE PROCESS COMPLAINTS',  # FS230 specific header\n",
        "        'Total_Records_in_File': n_records,\n",
        "        'File_Name': CONFIG['OUT_FILE'],\n",
        "        'File_Identifier': CONFIG['FILE_ID'],\n",
        "        'File_Reporting_Period': CONFIG['REPORT_PERIOD'],\n",
        "        'Filler': ''\n",
        "    }\n",
        "    return pd.DataFrame([header])\n",
        "\n",
        "def write_csv(header_df, data_df, filename):\n",
        "    \"\"\"Write final CSV (values only; no column headers).\"\"\"\n",
        "    # Write header row\n",
        "    header_csv = header_df.to_csv(index=False, header=False, lineterminator='\\r\\n')\n",
        "\n",
        "    # Write data rows\n",
        "    data_csv = data_df.to_csv(index=False, header=False, lineterminator='\\r\\n')\n",
        "\n",
        "    # Combine\n",
        "    final_csv = header_csv + data_csv\n",
        "\n",
        "    # Save to file\n",
        "    with open(filename, 'w', encoding='utf-8', newline='') as f:\n",
        "        f.write(final_csv)\n",
        "\n",
        "    return final_csv\n",
        "\n",
        "# =========================\n",
        "# QA Functions\n",
        "# =========================\n",
        "\n",
        "def perform_qa_checks(df_data):\n",
        "    \"\"\"Perform quality assurance checks on the data records.\"\"\"\n",
        "    qa_results = {}\n",
        "\n",
        "    # Check total record count\n",
        "    qa_results['total_records'] = len(df_data)\n",
        "    qa_results['detail_records'] = len(df_data[df_data['Total_Indicator'] == 'N'])\n",
        "    qa_results['eut_records'] = len(df_data[df_data['Total_Indicator'] == 'Y'])\n",
        "\n",
        "    # Check category distribution\n",
        "    qa_results['exp_status_count'] = len(df_data[df_data['EXP_Status'] != ''])\n",
        "    qa_results['exp_hrng_outcome_count'] = len(df_data[df_data['EXP_Hrng_Outcome'] != ''])\n",
        "    qa_results['exp_result_count'] = len(df_data[df_data['EXP_Result'] != ''])\n",
        "    qa_results['exp_rsltn_outcome_count'] = len(df_data[df_data['EXP_Rsltn_Outcome'] != ''])\n",
        "\n",
        "    # Check for missing values (-1)\n",
        "    qa_results['missing_count'] = len(df_data[df_data['EXP_Count'] == -1])\n",
        "    qa_results['zero_count'] = len(df_data[df_data['EXP_Count'] == 0])\n",
        "\n",
        "    # Verify exactly one category per detail row (except EUT)\n",
        "    detail_rows = df_data[df_data['Total_Indicator'] == 'N']\n",
        "    categories_per_row = []\n",
        "    for _, row in detail_rows.iterrows():\n",
        "        cat_count = sum([\n",
        "            1 if row['EXP_Status'] != '' else 0,\n",
        "            1 if row['EXP_Hrng_Outcome'] != '' else 0,\n",
        "            1 if row['EXP_Result'] != '' else 0,\n",
        "            1 if row['EXP_Rsltn_Outcome'] != '' else 0\n",
        "        ])\n",
        "        categories_per_row.append(cat_count)\n",
        "\n",
        "    qa_results['single_category_rows'] = categories_per_row.count(1)\n",
        "    qa_results['multi_category_rows'] = len([c for c in categories_per_row if c > 1])\n",
        "    qa_results['no_category_rows'] = categories_per_row.count(0)\n",
        "\n",
        "    # Check EUT calculation\n",
        "    status_rows = df_data[df_data['EXP_Status'] != '']\n",
        "    eut_row = df_data[df_data['Total_Indicator'] == 'Y'].iloc[0] if len(df_data[df_data['Total_Indicator'] == 'Y']) > 0 else None\n",
        "\n",
        "    if eut_row is not None:\n",
        "        if any(status_rows['EXP_Count'] == -1):\n",
        "            qa_results['eut_calculation'] = 'Correct (-1 due to missing)' if eut_row['EXP_Count'] == -1 else 'ERROR: Should be -1'\n",
        "        else:\n",
        "            expected_sum = status_rows['EXP_Count'].sum()\n",
        "            qa_results['eut_calculation'] = f'Correct (sum={expected_sum})' if eut_row['EXP_Count'] == expected_sum else f'ERROR: Expected {expected_sum}, got {eut_row[\"EXP_Count\"]}'\n",
        "\n",
        "    return qa_results\n",
        "\n",
        "# =========================\n",
        "# Main Execution\n",
        "# =========================\n",
        "\n",
        "def main(test_scenario='complete'):\n",
        "    \"\"\"\n",
        "    Main execution function.\n",
        "\n",
        "    Args:\n",
        "        test_scenario: Which test dataset to use\n",
        "            - 'complete': All sections present with valid data\n",
        "            - 'missing': Some sections missing to test gap-filling\n",
        "            - 'negatives': Test handling of negative/missing values\n",
        "            - 'edge_cases': Test duplicates and zero values\n",
        "            - 'realistic': Realistic expedited complaint counts\n",
        "            - 'upload': Upload your own file\n",
        "    \"\"\"\n",
        "    print(\"FS230 Expedited Due Process Complaints Build Script\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Validate configuration\n",
        "    validate_config()\n",
        "    print(\"✓ Configuration validated\")\n",
        "\n",
        "    # Select test data based on scenario\n",
        "    if test_scenario == 'complete':\n",
        "        print(\"\\nUsing COMPLETE test data (all sections present)...\")\n",
        "        df_input = create_test_data()\n",
        "    elif test_scenario == 'missing':\n",
        "        print(\"\\nUsing test data with MISSING sections...\")\n",
        "        df_input = create_test_data_with_missing()\n",
        "    elif test_scenario == 'negatives':\n",
        "        print(\"\\nUsing test data with NEGATIVE/MISSING values...\")\n",
        "        df_input = create_test_data_with_negatives()\n",
        "    elif test_scenario == 'edge_cases':\n",
        "        print(\"\\nUsing test data with EDGE CASES (duplicates, zeros)...\")\n",
        "        df_input = create_test_data_edge_cases()\n",
        "    elif test_scenario == 'realistic':\n",
        "        print(\"\\nUsing REALISTIC test data...\")\n",
        "        df_input = create_test_data_realistic()\n",
        "    elif test_scenario == 'upload':\n",
        "        print(\"\\nPlease upload your input CSV file...\")\n",
        "        uploaded = files.upload()\n",
        "        filename = list(uploaded.keys())[0]\n",
        "        df_input = pd.read_csv(filename)\n",
        "    else:\n",
        "        print(f\"\\nUnknown scenario '{test_scenario}'. Using complete test data...\")\n",
        "        df_input = create_test_data()\n",
        "\n",
        "    print(f\"\\nInput data shape: {df_input.shape}\")\n",
        "    print(\"\\nInput data preview:\")\n",
        "    print(df_input)\n",
        "\n",
        "    # Step 1: Normalize data\n",
        "    df_normalized = normalize_data(df_input)\n",
        "    print(\"\\n✓ Data normalized\")\n",
        "    print(\"\\nNormalized data:\")\n",
        "    print(df_normalized)\n",
        "\n",
        "    # Step 2: Build data records\n",
        "    df_data = build_data_records(df_normalized)\n",
        "    print(f\"\\n✓ Data records built: {len(df_data)} rows\")\n",
        "\n",
        "    # Step 3: Create header record\n",
        "    df_header = create_header_record(len(df_data))\n",
        "    print(\"✓ Header record created\")\n",
        "\n",
        "    # Step 4: Write CSV\n",
        "    output_filename = CONFIG['OUT_FILE']\n",
        "    write_csv(df_header, df_data, output_filename)\n",
        "    print(f\"\\n✓ CSV file written: {output_filename}\")\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"HEADER RECORD:\")\n",
        "    print(df_header.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"DATA RECORDS:\")\n",
        "    print(df_data.to_string(index=False))\n",
        "\n",
        "    # QA Summary\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"QA SUMMARY:\")\n",
        "    qa_results = perform_qa_checks(df_data)\n",
        "    for key, value in qa_results.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    # Check expected structure\n",
        "    print(\"\\nExpected Structure Validation:\")\n",
        "    expected_detail = 6  # FS230 has 6 detail rows\n",
        "    expected_total = 7   # 6 detail + 1 EUT\n",
        "\n",
        "    if qa_results['detail_records'] == expected_detail:\n",
        "        print(f\"  ✓ Detail records: {qa_results['detail_records']} (expected {expected_detail})\")\n",
        "    else:\n",
        "        print(f\"  ✗ Detail records: {qa_results['detail_records']} (expected {expected_detail})\")\n",
        "\n",
        "    if qa_results['total_records'] == expected_total:\n",
        "        print(f\"  ✓ Total records: {qa_results['total_records']} (expected {expected_total})\")\n",
        "    else:\n",
        "        print(f\"  ✗ Total records: {qa_results['total_records']} (expected {expected_total})\")\n",
        "\n",
        "    if qa_results['single_category_rows'] == expected_detail:\n",
        "        print(f\"  ✓ Single category per detail row: {qa_results['single_category_rows']}/{expected_detail}\")\n",
        "    else:\n",
        "        print(f\"  ✗ Single category per detail row: {qa_results['single_category_rows']}/{expected_detail}\")\n",
        "\n",
        "    # Category breakdown\n",
        "    print(\"\\nCategory Breakdown:\")\n",
        "    print(f\"  Status (Set A): {qa_results['exp_status_count']} rows\")\n",
        "    print(f\"  Hearing Outcome (Set B): {qa_results['exp_hrng_outcome_count']} rows\")\n",
        "    print(f\"  Result (Set D): {qa_results['exp_result_count']} rows\")\n",
        "    print(f\"  Resolution Outcome (Set C): {qa_results['exp_rsltn_outcome_count']} rows\")\n",
        "\n",
        "    # Value distribution\n",
        "    print(\"\\nEXP_Count Distribution:\")\n",
        "    print(df_data['EXP_Count'].value_counts().sort_index())\n",
        "\n",
        "    # Download the file\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Downloading output file...\")\n",
        "    files.download(output_filename)\n",
        "\n",
        "    return df_header, df_data\n",
        "\n",
        "# Run the main function with different test scenarios\n",
        "if __name__ == \"__main__\":\n",
        "    # Choose which test scenario to run:\n",
        "    # 'complete' - all sections with valid data\n",
        "    # 'missing' - some sections missing\n",
        "    # 'negatives' - test -1 handling\n",
        "    # 'edge_cases' - duplicates and zeros\n",
        "    # 'realistic' - realistic counts\n",
        "    # 'upload' - upload your own file\n",
        "\n",
        "    header_df, data_df = main(test_scenario='complete')\n",
        "\n",
        "    # Uncomment to test other scenarios:\n",
        "    # header_df, data_df = main(test_scenario='missing')\n",
        "    # header_df, data_df = main(test_scenario='negatives')\n",
        "    # header_df, data_df = main(test_scenario='edge_cases')\n",
        "    # header_df, data_df = main(test_scenario='realistic')\n",
        "    # header_df, data_df = main(test_scenario='upload')\n",
        "\n",
        "# =========================\n",
        "#  Python Program End\n",
        "# =========================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hcGYl_7k3kfw",
        "outputId": "0735ca03-5692-492b-a9a6-e7f3966cd84d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FS230 Expedited Due Process Complaints Build Script\n",
            "==================================================\n",
            "✓ Configuration validated\n",
            "\n",
            "Using COMPLETE test data (all sections present)...\n",
            "\n",
            "Input data shape: (6, 2)\n",
            "\n",
            "Input data preview:\n",
            "         SECTION  COUNT\n",
            "0     Legacy 4.2      4\n",
            "1     Legacy 4.3      5\n",
            "2     Legacy 4.4     70\n",
            "3  Legacy 4.2(a)      0\n",
            "4     Legacy 4.1     43\n",
            "5  Legacy 4.1(a)     21\n",
            "\n",
            "✓ Data normalized\n",
            "\n",
            "Normalized data:\n",
            "         SECTION  COUNT\n",
            "0     LEGACY 4.1     43\n",
            "1  LEGACY 4.1(A)     21\n",
            "2     LEGACY 4.2      4\n",
            "3  LEGACY 4.2(A)      0\n",
            "4     LEGACY 4.3      5\n",
            "5     LEGACY 4.4     70\n",
            "\n",
            "✓ Data records built: 7 rows\n",
            "✓ Header record created\n",
            "\n",
            "✓ CSV file written: txseaIDEABEDPCv000001.csv\n",
            "\n",
            "==================================================\n",
            "HEADER RECORD:\n",
            "                                  File_Type  Total_Records_in_File                 File_Name              File_Identifier File_Reporting_Period Filler\n",
            "SEA PART B EXPEDITED DUE PROCESS COMPLAINTS                      7 txseaIDEABEDPCv000001.csv FS230_TX_FFY2024_SY2425_V001             2024-2025       \n",
            "\n",
            "==================================================\n",
            "DATA RECORDS:\n",
            " File_Record_Number DG559_State_Code DG570_State_Agency_Number Filler1 Filler2       Table_Name Filler3    EXP_Status EXP_Hrng_Outcome  EXP_Result EXP_Rsltn_Outcome Total_Indicator                                                                                                               Explanation  EXP_Count\n",
            "                  1               48                        01                 IDEABDISPRESEDPC                                        EXPRSLTNMTG                                 N       Expedited due process complaints that resulted in expedited resolution meetings by the end of the reporting period.         43\n",
            "                  2               48                        01                 IDEABDISPRESEDPC                                                      EXPWRTNSTMTAGMT               N          Expedited resolution meetings that resulted in written settlement agreements by the end of the reporting period.         21\n",
            "                  3               48                        01                 IDEABDISPRESEDPC         EXPHRNGADJCTD                                                              N Expedited due process complaints resulting in expedited hearings fully adjudicated as of the end of the reporting period.          4\n",
            "                  4               48                        01                 IDEABDISPRESEDPC                         CHNGPLCMTORDRD                                             N                                                    Expedited hearings fully adjudicated with change of placement ordered.          0\n",
            "                  5               48                        01                 IDEABDISPRESEDPC             EXPDPCPND                                                              N                                           Expedited due process complaints pending as of the end of the reporting period.          5\n",
            "                  6               48                        01                 IDEABDISPRESEDPC          EXPDPCWTHDRW                                                              N                            Expedited due process complaints withdrawn or dismissed as of the end of the reporting period.         70\n",
            "                  7               48                        01                 IDEABDISPRESEDPC                                                                                    Y                                                                                   Total expedited due process complaints.         79\n",
            "\n",
            "==================================================\n",
            "QA SUMMARY:\n",
            "  total_records: 7\n",
            "  detail_records: 6\n",
            "  eut_records: 1\n",
            "  exp_status_count: 3\n",
            "  exp_hrng_outcome_count: 1\n",
            "  exp_result_count: 1\n",
            "  exp_rsltn_outcome_count: 1\n",
            "  missing_count: 0\n",
            "  zero_count: 1\n",
            "  single_category_rows: 6\n",
            "  multi_category_rows: 0\n",
            "  no_category_rows: 0\n",
            "  eut_calculation: Correct (sum=79)\n",
            "\n",
            "Expected Structure Validation:\n",
            "  ✓ Detail records: 6 (expected 6)\n",
            "  ✓ Total records: 7 (expected 7)\n",
            "  ✓ Single category per detail row: 6/6\n",
            "\n",
            "Category Breakdown:\n",
            "  Status (Set A): 3 rows\n",
            "  Hearing Outcome (Set B): 1 rows\n",
            "  Result (Set D): 1 rows\n",
            "  Resolution Outcome (Set C): 1 rows\n",
            "\n",
            "EXP_Count Distribution:\n",
            "EXP_Count\n",
            "0     1\n",
            "4     1\n",
            "5     1\n",
            "21    1\n",
            "43    1\n",
            "70    1\n",
            "79    1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "==================================================\n",
            "Downloading output file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_116e3bce-8a64-47f6-9acd-247ad240d98d\", \"txseaIDEABEDPCv000001.csv\", 1087)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Contact**\n",
        "\n",
        "For questions or to report errors, email [zane.wubbena@tea.texas.gov](zane.wubbena@tea.texas.gov)"
      ],
      "metadata": {
        "id": "8Gi03YUNCaYh"
      }
    }
  ]
}